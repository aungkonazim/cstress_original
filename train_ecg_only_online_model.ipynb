{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "path_to_zip_file = 'data.zip'\n",
    "directory_to_extract_to = './data/'\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process raw ecg data, clean, R peak detection, outlier r peak removal, feature computation and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 240 (240, 9) ./data/ ./data/SI01/ (240, 9) (237, 9)\n",
      "287 287 (287, 9) ./data/ ./data/SI02/ (287, 9) (284, 9)\n",
      "256 256 (256, 9) ./data/ ./data/SI03/ (256, 9) (253, 9)\n",
      "244 244 (244, 9) ./data/ ./data/SI04/ (244, 9) (241, 9)\n",
      "263 263 (263, 9) ./data/ ./data/SI06/ (263, 9) (260, 9)\n",
      "279 279 (279, 9) ./data/ ./data/SI07/ (279, 9) (276, 9)\n",
      "127 127 (127, 9) ./data/ ./data/SI08/ (127, 9) (124, 9)\n",
      "271 271 (271, 9) ./data/ ./data/SI10/ (271, 9) (268, 9)\n",
      "261 261 (261, 9) ./data/ ./data/SI12/ (261, 9) (258, 9)\n",
      "265 265 (265, 9) ./data/ ./data/SI13/ (265, 9) (262, 9)\n",
      "301 301 (301, 9) ./data/ ./data/SI15/ (301, 9) (298, 9)\n",
      "276 276 (276, 9) ./data/ ./data/SI16/ (276, 9) (273, 9)\n",
      "267 267 (267, 9) ./data/ ./data/SI17/ (267, 9) (264, 9)\n",
      "259 259 (259, 9) ./data/ ./data/SI18/ (259, 9) (256, 9)\n",
      "260 260 (260, 9) ./data/ ./data/SI19/ (260, 9) (257, 9)\n",
      "263 263 (263, 9) ./data/ ./data/SI20/ (263, 9) (260, 9)\n",
      "260 260 (260, 9) ./data/ ./data/SI21/ (260, 9) (257, 9)\n",
      "255 255 (255, 9) ./data/ ./data/SI22/ (255, 9) (252, 9)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tomkin import detect_rpeak\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from outlier_calculation import Quality,compute_outlier_ecg\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel,delayed\n",
    "from data_quality import ECGQualityCalculation\n",
    "from joblib import delayed,Parallel\n",
    "from copy import deepcopy\n",
    "from ecg import ecg_feature_computation\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,QuantileTransformer\n",
    "import gzip\n",
    "from scipy import stats\n",
    "\n",
    "def get_interpolated(aclx,acly,aclz):\n",
    "    time_array = aclx[:,1].reshape(-1,1)\n",
    "    aclxyz = np.concatenate([time_array,time_array,time_array,time_array],axis=1)\n",
    "    f = interpolate.interp1d(aclx[:,1],aclx[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,1] = f(aclxyz[:,0])\n",
    "    f = interpolate.interp1d(acly[:,1],acly[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,2] = f(aclxyz[:,0])\n",
    "    f = interpolate.interp1d(aclz[:,1],aclz[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,3] = f(aclxyz[:,0])\n",
    "    return aclxyz\n",
    "\n",
    "\n",
    "def get_clean_ecg(ecg_data):\n",
    "    final_data = np.zeros((0,3))\n",
    "    if len(ecg_data)==0:\n",
    "        return final_data\n",
    "    test_object = ECGQualityCalculation()\n",
    "    start_ts = ecg_data[0,0]\n",
    "    final_data = np.zeros((0,3))\n",
    "    while start_ts<ecg_data[-1,0]:\n",
    "        index = np.where((ecg_data[:,0]>=start_ts)&(ecg_data[:,0]<start_ts+3000))[0]\n",
    "        temp_data = ecg_data[index,2]\n",
    "        if test_object.current_quality(temp_data)==1:\n",
    "            final_data = np.concatenate((final_data,ecg_data[index,:]))\n",
    "        start_ts = start_ts + 3000\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def get_hr(ecg_data):\n",
    "#     try:\n",
    "    rpeaks = detect_rpeak(ecg_data[:,2],64)\n",
    "    rpeak_ts = ecg_data[rpeaks,0]\n",
    "    ecg_rr = np.zeros((len(rpeaks)-1,2))\n",
    "    ecg_rr_ts = np.array(rpeak_ts)[1:]\n",
    "    ecg_rr_sample = np.array(np.diff(rpeak_ts))\n",
    "    index = np.where((ecg_rr_sample>=300)&(ecg_rr_sample<=2000))[0]\n",
    "    ecg_rr_ts = ecg_rr_ts[index]\n",
    "    ecg_rr_sam = ecg_rr_sample[index]\n",
    "#     rr = remove_ectopic_beats(ecg_rr_sam)\n",
    "    rr = ecg_rr_sam\n",
    "    ecg_rr_sam = ecg_rr_sam[~np.isnan(rr)]\n",
    "    ecg_rr_ts = ecg_rr_ts[~np.isnan(rr)]\n",
    "    outlier = compute_outlier_ecg(ecg_rr_ts/1000,ecg_rr_sam/1000)\n",
    "    ind1 = []\n",
    "    for ind,tup in enumerate(outlier):\n",
    "        if tup[1]==Quality.ACCEPTABLE:\n",
    "            ind1.append(ind)\n",
    "    ind1 = np.array(ind1)\n",
    "    if len(ind1)<100:\n",
    "        return [],[]\n",
    "    ecg_rr_ts = ecg_rr_ts[ind1]\n",
    "    ecg_rr_sam = ecg_rr_sam[ind1]\n",
    "    return ecg_rr_ts,ecg_rr_sam\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "\n",
    "\n",
    "\n",
    "def get_2sec_rr(data):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],2000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>t-2500)&(data[:,0]<=t+2500))[0]\n",
    "        if len(index)==0:\n",
    "            continue\n",
    "        window_col.append(np.array([t,np.mean(data[index,1])]))\n",
    "    return np.array(window_col)\n",
    "\n",
    "def get_windows(data,window_size=10,offset=10,fs=1):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>t-window_size*1000/2)&(data[:,0]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<10:\n",
    "            continue\n",
    "        window = data[index,:]\n",
    "        window[:,1] = stats.mstats.winsorize(window[:,1],limits=.02)\n",
    "        window_col.append(data[index,:])\n",
    "    return window_col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_std_chest(window,start=1,end=4):\n",
    "    return np.array([np.mean(window[:,0]),np.sqrt(np.sum(np.power(np.std(window[:,start:end],axis=0),2)))])\n",
    "\n",
    "\n",
    "def filter_ecg_windows(ecg_windows,acl_std):\n",
    "    final_ecg_windows = []\n",
    "    for window in ecg_windows:\n",
    "        index = np.where((acl_std[:,0]>window[0,0])&(acl_std[:,0]<window[-1,0]))[0]\n",
    "        if len(index)==0:\n",
    "            continue\n",
    "        window_temp = acl_std[index,1].reshape(-1)\n",
    "        if len(window_temp[window_temp>.21])/len(window_temp) > .5:\n",
    "            continue\n",
    "        final_ecg_windows.append(window)\n",
    "    return final_ecg_windows\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class OnlineVariance(object):\n",
    "    \"\"\"\n",
    "    Welford's algorithm computes the sample variance incrementally.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iterable=None, ddof=1):\n",
    "        self.ddof, self.n, self.mean, self.M2 = ddof, 0, 0.0, 0.0\n",
    "        if iterable is not None:\n",
    "            for datum in iterable:\n",
    "                self.include(datum)\n",
    "\n",
    "    def include(self, datum):\n",
    "        self.n += 1\n",
    "        self.delta = datum - self.mean\n",
    "        self.mean += self.delta / self.n\n",
    "        self.M2 += self.delta * (datum - self.mean)\n",
    "\n",
    "    @property\n",
    "    def variance(self):\n",
    "        return self.M2 / (self.n - self.ddof)\n",
    "\n",
    "    @property\n",
    "    def std(self):\n",
    "        return np.sqrt(self.variance)\n",
    "    \n",
    "    \n",
    "    \n",
    "# from __future__ import division\n",
    "import numpy\n",
    "\n",
    "\n",
    "class incre_std_avg:\n",
    "    '''\n",
    "    Incremental calculation of the average value, standard deviation, and variance of massive data\n",
    "         1. Data\n",
    "         obj.avg is the average\n",
    "         obj.std is the standard deviation\n",
    "         obj.n is the number of data\n",
    "         When the object is initialized, you need to specify the historical average, historical standard deviation, and the number of historical data (the initial data set is empty if you don't need to fill in)\n",
    "         2. Method\n",
    "         The obj.incre_in_value() method passes in a new data to be calculated, performs incremental calculations, and obtains new avg, std and n (for massive data, please bring each new parameter loop into this method)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, h_avg=0, h_std=0, n=0):\n",
    "        self.avg = h_avg\n",
    "        self.std = h_std\n",
    "        self.n = n\n",
    "\n",
    "    def incre_in_value(self, value):\n",
    "        incre_avg = (self.n*self.avg+value)/(self.n+1)\n",
    "        incre_std = numpy.sqrt((self.n*(self.std**2+(incre_avg-self.avg)\n",
    "                                        ** 2)+(incre_avg-value)**2)/(self.n+1))\n",
    "        self.avg = incre_avg\n",
    "        self.std = incre_std\n",
    "        self.n += 1\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     c = incre_std_avg()\n",
    "#     c.incre_in_value(0.05)\n",
    "#     print c.avg\n",
    "#     print c.std\n",
    "#     print c.n\n",
    "#     c.incre_in_value(0.02)\n",
    "#     c.incre_in_list([0.5, 0.2, 0.3])\n",
    "#     print c.avg\n",
    "#     print c.std\n",
    "#     print c.n\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "path = './data/'\n",
    "participants = [path + f +'/' for f in os.listdir(path) if f[0]=='S']\n",
    "for f in participants:\n",
    "    if 'ecg.txt.gz' not in os.listdir(f):\n",
    "        continue\n",
    "    st = 0\n",
    "    et = 0 \n",
    "    with gzip.open(f+'stress_marks.txt.gz', 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.decode('utf8').strip()\n",
    "            parts = [x.strip() for x in line.split(',')]\n",
    "            label = parts[0]\n",
    "            if label[:2] in ['c1']:\n",
    "                st = np.int64(parts[2])\n",
    "                et = np.int64(parts[3])\n",
    "#     aclx = pd.read_csv(f +'accelx.txt.gz', compression='gzip',\n",
    "#                           sep=' ',header=None).values\n",
    "#     acly = pd.read_csv(f +'accely.txt.gz', compression='gzip',\n",
    "#                           sep=' ',header=None).values\n",
    "#     aclz = pd.read_csv(f +'accelz.txt.gz', compression='gzip',\n",
    "#                           sep=' ',header=None).values\n",
    "#     acl_all = get_interpolated(aclx,acly,aclz)\n",
    "#     aclx = ((3*acl_all[:,1].reshape(-1)/4095) - 1.5) / 0.3\n",
    "#     acly = ((3*acl_all[:,2].reshape(-1)/4095) - 1.5) / 0.3\n",
    "#     aclz = ((3*acl_all[:,3].reshape(-1)/4095) - 1.5) / 0.3\n",
    "#     aclxyz = np.concatenate([acl_all[:,0].reshape(-1,1),aclx.reshape(-1,1),acly.reshape(-1,1),aclz.reshape(-1,1)],axis=1)\n",
    "#     acl_windows = get_windows(aclxyz,window_size=10,offset=10,fs=15)\n",
    "#     acl_std = np.array([get_std_chest(window) for window in acl_windows])\n",
    "#     ecg_temp = pd.read_csv(f +'ecg.txt.gz', compression='gzip',\n",
    "#                           sep=' ',header=None).values\n",
    "#     print(ecg_temp.shape,f)\n",
    "#     ecg = np.zeros((ecg_temp.shape[0],ecg_temp.shape[1]+1))\n",
    "#     ecg[:,0],ecg[:,2] = ecg_temp[:,1],ecg_temp[:,0]\n",
    "#     ecg_rr_ts,ecg_rr_sam = get_hr(ecg)\n",
    "#     if len(ecg_rr_sam)<100:\n",
    "#         continue\n",
    "#     print(ecg_rr_ts.shape,f)\n",
    "#     ecg_rr = np.zeros((len(ecg_rr_ts),2))\n",
    "#     ecg_rr[:,0] = ecg_rr_ts\n",
    "#     ecg_rr[:,1] = ecg_rr_sam\n",
    "#     pickle.dump(ecg_rr,open(f+'ecg_rr.p','wb'))\n",
    "    \n",
    "    if os.path.isfile(f+'ecg_rr.p') and st>0:\n",
    "        ecg_rr = pickle.load(open(f+'ecg_rr.p','rb'))\n",
    "        ecg_rr = get_2sec_rr(ecg_rr)\n",
    "        ecg_rr[:,1] = 60000/ecg_rr[:,1]\n",
    "        ecg_windows = get_windows(ecg_rr,window_size=60,offset=30,fs=1)\n",
    "        final_ecg_windows = ecg_windows\n",
    "        ecg_features = np.array([np.array([window[0,0],window[-1,0]]+ecg_feature_computation(window[:,0],window[:,1])[:-4]) for window in final_ecg_windows])\n",
    "        ecg_features = ecg_features[ecg_features[:,0].argsort(),:]\n",
    "        n_features = 7\n",
    "        mean_std_class_array = [incre_std_avg() for i in range(n_features)]\n",
    "        N = 3\n",
    "        final_features = []\n",
    "        for i in range(1,ecg_features.shape[0]):\n",
    "            feature_array = ecg_features[i]\n",
    "            st,et,features = feature_array[0],feature_array[1],np.array(feature_array[2:])\n",
    "            normalized_feature_array = [st,et]\n",
    "            for k,value in enumerate(features):\n",
    "                mean_std_class_array[k].incre_in_value(value)\n",
    "                if i<N:\n",
    "                    continue\n",
    "                avg, std = mean_std_class_array[k].avg,mean_std_class_array[k].std\n",
    "                value = (value-avg)/std\n",
    "                normalized_feature_array.append(value)\n",
    "            if len(normalized_feature_array)==n_features+2:\n",
    "                final_features.append(np.array(normalized_feature_array))\n",
    "\n",
    "        ecg_features_normalized = np.array(final_features)\n",
    "        print(len(ecg_windows),len(final_ecg_windows),ecg_features.shape,path,f,ecg_features.shape,ecg_features_normalized.shape)\n",
    "        pickle.dump(ecg_features_normalized,open(f+'features22norm.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store' 'SI01' 'SI02' 'SI03' 'SI04' 'SI05' 'SI06' 'SI07' 'SI08'\n",
      " 'SI09' 'SI10' 'SI11' 'SI12' 'SI13' 'SI14' 'SI15' 'SI16' 'SI17' 'SI18'\n",
      " 'SI19' 'SI20' 'SI21' 'SI22' 'SI23' 'SI24' '.ipynb_checkpoints'\n",
      " 'feature.csv' 'feature_rip.csv' 'feature_ecg.csv' 'feature_all.csv'\n",
      " 'feature_rip_ecg.csv' 'feature_ecg_norm.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2597, 11)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soujanya Chatterjee\n",
    "# 2:06 PM (9 minutes ago)\n",
    "\t\n",
    "# to me\n",
    "import pandas as pd, numpy as np, os, csv, glob, math, matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from datetime import datetime\n",
    "from scipy.stats import *\n",
    "import gzip\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def find_majority(k):\n",
    "    myMap = {}\n",
    "    maximum = ( '', 0 ) # (occurring element, occurrences)\n",
    "    for n in k:\n",
    "        if n in myMap: myMap[n] += 1\n",
    "        else: myMap[n] = 1\n",
    "\n",
    "        # Keep track of maximum on the go\n",
    "        if myMap[n] > maximum[1]: maximum = (n,myMap[n])\n",
    "\n",
    "    return maximum[0]\n",
    "\n",
    "# _dir = 'W:\\\\Students\\\\cstress_features\\\\data\\\\data\\\\SI02\\\\'\n",
    "\n",
    "def decodeLabel(label):\n",
    "    label = label[:2]  # Only the first 2 characters designate the label code\n",
    "\n",
    "    mapping = {'c1': 0, 'c2': 1, 'c3': 1, 'c4': 0, 'c5': 0, 'c6': 0, 'c7': 2}\n",
    "\n",
    "    return mapping[label]\n",
    "\n",
    "def readstressmarks(participantID, filename):\n",
    "    features = []\n",
    "    for file in os.listdir(filename):    \n",
    "        if file.endswith(\"marks.txt.gz\"):        \n",
    "            with gzip.open(os.path.join(filename, file), 'r') as file:\n",
    "                for line in file.readlines():\n",
    "                    line = line.decode('utf8').strip()\n",
    "                    parts = [x.strip() for x in line.split(',')]                    \n",
    "                    label = parts[0][:2]  \n",
    "                    if label not in ['c7','c6']:\n",
    "                        stressClass = decodeLabel(label)\n",
    "                        features.append([participantID, stressClass, int(parts[2]), int(parts[3])])\n",
    "    return np.array(features)\n",
    "\n",
    "_dirr = './data/'\n",
    "parti = np.array(os.listdir(_dirr) )\n",
    "print(parti)\n",
    "header = ['participant','starttime','endtime','label','f_1','f_2','f_3','f_4','f_5','f_6','f_7']\n",
    "fea_cols = ['f_1','f_2','f_3','f_4','f_5','f_6','f_7']\n",
    "data = []\n",
    "for p in parti:\n",
    "    if p in ['feature.csv','feature_ecg.csv','feature_rip.csv','feature_rip_ecg.csv','feature_all.csv','SI05','SI09','SI11','SI14','SI23','SI24','.ipynb_checkpoints']:\n",
    "        continue\n",
    "    else:\n",
    "        if os.path.isdir(os.path.join(_dirr,p)):\n",
    "           \n",
    "            _dir = (os.path.join(_dirr,p))\n",
    "            gt_marks = readstressmarks(p,_dir)\n",
    "            groundtruth = pd.DataFrame({'participant': gt_marks[:, 0], 'label': gt_marks[:, 1], 'starttime': gt_marks[:, 2],\n",
    "                                        'endtime': gt_marks[:, 3]}, columns=['participant','label','starttime','endtime'])\n",
    "            groundtruth = groundtruth.sort_values('starttime')\n",
    "   \n",
    "            x = []\n",
    "            for file in os.listdir(_dir):    \n",
    "                    if file.endswith(\"22norm.p\"):                    \n",
    "                        with open(_dir+'/'+file, 'rb') as f:  \n",
    "                            x = pickle.load(f)\n",
    "            if len(x)==0:\n",
    "                continue\n",
    "#             print(x)\n",
    "            dataset = pd.DataFrame({'starttime': x[:, 0], 'endtime': x[:, 1], 'f_1': x[:, 2]\n",
    "                                   , 'f_2': x[:, 3], 'f_3': x[:, 4], 'f_4': x[:, 5]\n",
    "                                   , 'f_5': x[:, 6], 'f_6': x[:, 7], 'f_7': x[:, 8]}, columns=['starttime','endtime','f_1','f_2',\n",
    "                                                                                               'f_3','f_4','f_5','f_6','f_7'])\n",
    "           \n",
    "            dataset = dataset.sort_values('starttime')\n",
    "\n",
    "            for gt in range(len(dataset)):\n",
    "                starttime = int(dataset['starttime'].iloc[gt])\n",
    "                endtime = int(dataset['endtime'].iloc[gt])\n",
    "                result = []\n",
    "                for line in range(len(groundtruth)):\n",
    "                    id, gtt, st, et = [groundtruth['participant'].iloc[line], groundtruth['label'].iloc[line], int(groundtruth['starttime'].iloc[line]),\n",
    "                                      int(groundtruth['endtime'].iloc[line])]\n",
    "                    if starttime < st:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (starttime > st) and (endtime < et):\n",
    "                            result.append(gtt)\n",
    "                        if result:\n",
    "                            fea = list(dataset[fea_cols].iloc[gt])\n",
    "                            inter_data = [p, st,et,find_majority(result)],(fea)\n",
    "                            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "                            data.append(flatten(inter_data))\n",
    "    #         print(data)\n",
    "df = pd.DataFrame(data)\n",
    "df.fillna(df.mean(),inplace=True)\n",
    "df.to_csv(_dirr + '/' + 'feature_ecg_norm.csv', index=False, header=header)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2597, 7) (2597,) 493\n"
     ]
    }
   ],
   "source": [
    "feature_file = './data/feature_ecg_norm.csv'\n",
    "feature = pd.read_csv(feature_file).values\n",
    "y = np.int64(feature[:,3])\n",
    "X = feature[:,4:]\n",
    "print(X.shape,y.shape,np.sum(y))\n",
    "groups = feature[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 18 folds for each of 240 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:    9.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:   11.4s\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:   27.0s\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:   48.1s\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed:   58.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.913):\n",
      "{'C': 11.885022274370183, 'class_weight': {0: 0.4, 1: 0.6}, 'gamma': 0.1767766952966369, 'kernel': 'rbf', 'probability': False}\n",
      "[[2005   99]\n",
      " [ 125  368]]               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      2104\n",
      "           1       0.79      0.75      0.77       493\n",
      "\n",
      "    accuracy                           0.91      2597\n",
      "   macro avg       0.86      0.85      0.86      2597\n",
      "weighted avg       0.91      0.91      0.91      2597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "paramGrid = {'kernel': ['rbf'],\n",
    "             'C': np.logspace(.1,4,5),\n",
    "             'gamma': [np.power(2,np.float(x)) for x in np.arange(-4, 4, .5)],\n",
    "             'class_weight': [{0: w, 1: 1 - w} for w in [.4,.3,.2]],\n",
    "             'probability':[False]\n",
    "}\n",
    "# clf = Pipeline([('rf', SVC())])\n",
    "clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='f1_weighted',verbose=5)\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups))\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=11.885022274370183, break_ties=False, cache_size=200,\n",
      "    class_weight={0: 0.4, 1: 0.6}, coef0=0.0, decision_function_shape='ovr',\n",
      "    degree=3, gamma=0.1767766952966369, kernel='rbf', max_iter=-1,\n",
      "    probability=True, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "clf.set_params(probability=True)\n",
    "print(clf)\n",
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/moods/stress_model.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "clf.set_params(probability=True)\n",
    "print(clf)\n",
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/cc3/ecg_model_feature_standardization.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_porter import Porter\n",
    "porter = Porter(clf, language='java')\n",
    "output = porter.export(export_data=True)\n",
    "# print(output)\n",
    "text_file = open(\"SVM1.java\", \"w\")\n",
    "text_file.write(output)\n",
    "text_file.close()\n",
    "print(clf.probA_,clf.probB_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_porter import Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
