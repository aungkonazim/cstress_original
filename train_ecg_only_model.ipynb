{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "path_to_zip_file = 'data.zip'\n",
    "directory_to_extract_to = './data/'\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process raw ecg data, clean, R peak detection, outlier r peak removal, feature computation and standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458149, 2) ./data/SI01/\n",
      "(7412,) ./data/SI01/\n",
      "238 238 (238, 13) ./data/ ./data/SI01/ (238, 13)\n",
      "(550199, 2) ./data/SI02/\n",
      "(9605,) ./data/SI02/\n",
      "283 283 (283, 13) ./data/ ./data/SI02/ (283, 13)\n",
      "(476149, 2) ./data/SI03/\n",
      "(9333,) ./data/SI03/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:72: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lfhfRation_log = np.log((lf_power / hf_power) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 255 (255, 13) ./data/ ./data/SI03/ (255, 13)\n",
      "(460799, 2) ./data/SI04/\n",
      "(6911,) ./data/SI04/\n",
      "244 244 (244, 13) ./data/ ./data/SI04/ (244, 13)\n",
      "(561649, 2) ./data/SI05/\n",
      "(492299, 2) ./data/SI06/\n",
      "(10504,) ./data/SI06/\n",
      "260 260 (260, 13) ./data/ ./data/SI06/ (260, 13)\n",
      "(505699, 2) ./data/SI07/\n",
      "(7660,) ./data/SI07/\n",
      "279 279 (279, 13) ./data/ ./data/SI07/ (279, 13)\n",
      "(476199, 2) ./data/SI08/\n",
      "(4208,) ./data/SI08/\n",
      "126 126 (126, 13) ./data/ ./data/SI08/ (126, 13)\n",
      "(506049, 2) ./data/SI10/\n",
      "(8960,) ./data/SI10/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:72: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lfhfRation_log = np.log((lf_power / hf_power) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "271 271 (271, 13) ./data/ ./data/SI10/ (271, 13)\n",
      "(584349, 2) ./data/SI11/\n",
      "(496649, 2) ./data/SI12/\n",
      "(8295,) ./data/SI12/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:72: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lfhfRation_log = np.log((lf_power / hf_power) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 261 (261, 13) ./data/ ./data/SI12/ (261, 13)\n",
      "(491899, 2) ./data/SI13/\n",
      "(7844,) ./data/SI13/\n",
      "261 261 (261, 13) ./data/ ./data/SI13/ (261, 13)\n",
      "(543949, 2) ./data/SI14/\n",
      "(569999, 2) ./data/SI15/\n",
      "(11280,) ./data/SI15/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:72: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lfhfRation_log = np.log((lf_power / hf_power) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 300 (300, 13) ./data/ ./data/SI15/ (300, 13)\n",
      "(516749, 2) ./data/SI16/\n",
      "(9200,) ./data/SI16/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:72: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lfhfRation_log = np.log((lf_power / hf_power) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 277 (277, 13) ./data/ ./data/SI16/ (277, 13)\n",
      "(502349, 2) ./data/SI17/\n",
      "(8333,) ./data/SI17/\n",
      "266 266 (266, 13) ./data/ ./data/SI17/ (266, 13)\n",
      "(478649, 2) ./data/SI18/\n",
      "(8379,) ./data/SI18/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:72: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lfhfRation_log = np.log((lf_power / hf_power) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255 255 (255, 13) ./data/ ./data/SI18/ (255, 13)\n",
      "(491799, 2) ./data/SI19/\n",
      "(6035,) ./data/SI19/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:71: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:72: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
      "/home/jupyter/mullah/Test/data_yield/cstress_features/ecg.py:74: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lfhfRation_log = np.log((lf_power / hf_power) + 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258 258 (258, 13) ./data/ ./data/SI19/ (258, 13)\n",
      "(509099, 2) ./data/SI20/\n",
      "(6659,) ./data/SI20/\n",
      "263 263 (263, 13) ./data/ ./data/SI20/ (263, 13)\n",
      "(489149, 2) ./data/SI21/\n",
      "(8984,) ./data/SI21/\n",
      "260 260 (260, 13) ./data/ ./data/SI21/ (260, 13)\n",
      "(472599, 2) ./data/SI22/\n",
      "(10373,) ./data/SI22/\n",
      "255 255 (255, 13) ./data/ ./data/SI22/ (255, 13)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tomkin import detect_rpeak\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from outlier_calculation import Quality,compute_outlier_ecg\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel,delayed\n",
    "from data_quality import ECGQualityCalculation\n",
    "from joblib import delayed,Parallel\n",
    "from copy import deepcopy\n",
    "from ecg import ecg_feature_computation\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,QuantileTransformer\n",
    "import gzip\n",
    "from scipy import stats\n",
    "\n",
    "def get_interpolated(aclx,acly,aclz):\n",
    "    time_array = aclx[:,1].reshape(-1,1)\n",
    "    aclxyz = np.concatenate([time_array,time_array,time_array,time_array],axis=1)\n",
    "    f = interpolate.interp1d(aclx[:,1],aclx[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,1] = f(aclxyz[:,0])\n",
    "    f = interpolate.interp1d(acly[:,1],acly[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,2] = f(aclxyz[:,0])\n",
    "    f = interpolate.interp1d(aclz[:,1],aclz[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,3] = f(aclxyz[:,0])\n",
    "    return aclxyz\n",
    "\n",
    "\n",
    "def get_clean_ecg(ecg_data):\n",
    "    final_data = np.zeros((0,3))\n",
    "    if len(ecg_data)==0:\n",
    "        return final_data\n",
    "    test_object = ECGQualityCalculation()\n",
    "    start_ts = ecg_data[0,0]\n",
    "    final_data = np.zeros((0,3))\n",
    "    while start_ts<ecg_data[-1,0]:\n",
    "        index = np.where((ecg_data[:,0]>=start_ts)&(ecg_data[:,0]<start_ts+3000))[0]\n",
    "        temp_data = ecg_data[index,2]\n",
    "        if test_object.current_quality(temp_data)==1:\n",
    "            final_data = np.concatenate((final_data,ecg_data[index,:]))\n",
    "        start_ts = start_ts + 3000\n",
    "    return final_data\n",
    "\n",
    "\n",
    "def get_hr(ecg_data):\n",
    "#     try:\n",
    "    rpeaks = detect_rpeak(ecg_data[:,2],64)\n",
    "    rpeak_ts = ecg_data[rpeaks,0]\n",
    "    ecg_rr = np.zeros((len(rpeaks)-1,2))\n",
    "    ecg_rr_ts = np.array(rpeak_ts)[1:]\n",
    "    ecg_rr_sample = np.array(np.diff(rpeak_ts))\n",
    "    index = np.where((ecg_rr_sample>=300)&(ecg_rr_sample<=2000))[0]\n",
    "    ecg_rr_ts = ecg_rr_ts[index]\n",
    "    ecg_rr_sam = ecg_rr_sample[index]\n",
    "#     rr = remove_ectopic_beats(ecg_rr_sam)\n",
    "    rr = ecg_rr_sam\n",
    "    ecg_rr_sam = ecg_rr_sam[~np.isnan(rr)]\n",
    "    ecg_rr_ts = ecg_rr_ts[~np.isnan(rr)]\n",
    "    outlier = compute_outlier_ecg(ecg_rr_ts/1000,ecg_rr_sam/1000)\n",
    "    ind1 = []\n",
    "    for ind,tup in enumerate(outlier):\n",
    "        if tup[1]==Quality.ACCEPTABLE:\n",
    "            ind1.append(ind)\n",
    "    ind1 = np.array(ind1)\n",
    "    if len(ind1)<100:\n",
    "        return [],[]\n",
    "    ecg_rr_ts = ecg_rr_ts[ind1]\n",
    "    ecg_rr_sam = ecg_rr_sam[ind1]\n",
    "    return ecg_rr_ts,ecg_rr_sam\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "\n",
    "\n",
    "def get_windows(data,window_size=10,offset=10,fs=1):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>t-window_size*1000/2)&(data[:,0]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<30:\n",
    "            continue\n",
    "        window_col.append(data[index,:])\n",
    "    return window_col\n",
    "\n",
    "def get_std_chest(window,start=1,end=4):\n",
    "    return np.array([np.mean(window[:,0]),np.sqrt(np.sum(np.power(np.std(window[:,start:end],axis=0),2)))])\n",
    "\n",
    "\n",
    "def filter_ecg_windows(ecg_windows,acl_std):\n",
    "    final_ecg_windows = []\n",
    "    for window in ecg_windows:\n",
    "        index = np.where((acl_std[:,0]>window[0,0])&(acl_std[:,0]<window[-1,0]))[0]\n",
    "        if len(index)==0:\n",
    "            continue\n",
    "        window_temp = acl_std[index,1].reshape(-1)\n",
    "        if len(window_temp[window_temp>.21])/len(window_temp) > .5:\n",
    "            continue\n",
    "        final_ecg_windows.append(window)\n",
    "    return final_ecg_windows\n",
    "path = './data/'\n",
    "participants = [path + f +'/' for f in os.listdir(path) if f[0]=='S']\n",
    "for f in participants:\n",
    "    if 'ecg.txt.gz' not in os.listdir(f):\n",
    "        continue\n",
    "    st = 0\n",
    "    et = 0 \n",
    "    with gzip.open(f+'stress_marks.txt.gz', 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.decode('utf8').strip()\n",
    "            parts = [x.strip() for x in line.split(',')]\n",
    "            label = parts[0]\n",
    "            if label[:2] in ['c1']:\n",
    "                st = np.int64(parts[2])\n",
    "                et = np.int64(parts[3])\n",
    "    aclx = pd.read_csv(f +'accelx.txt.gz', compression='gzip',\n",
    "                          sep=' ',header=None).values\n",
    "    acly = pd.read_csv(f +'accely.txt.gz', compression='gzip',\n",
    "                          sep=' ',header=None).values\n",
    "    aclz = pd.read_csv(f +'accelz.txt.gz', compression='gzip',\n",
    "                          sep=' ',header=None).values\n",
    "    acl_all = get_interpolated(aclx,acly,aclz)\n",
    "    aclx = ((3*acl_all[:,1].reshape(-1)/4095) - 1.5) / 0.3\n",
    "    acly = ((3*acl_all[:,2].reshape(-1)/4095) - 1.5) / 0.3\n",
    "    aclz = ((3*acl_all[:,3].reshape(-1)/4095) - 1.5) / 0.3\n",
    "    aclxyz = np.concatenate([acl_all[:,0].reshape(-1,1),aclx.reshape(-1,1),acly.reshape(-1,1),aclz.reshape(-1,1)],axis=1)\n",
    "    acl_windows = get_windows(aclxyz,window_size=10,offset=10,fs=15)\n",
    "    acl_std = np.array([get_std_chest(window) for window in acl_windows])\n",
    "    ecg_temp = pd.read_csv(f +'ecg.txt.gz', compression='gzip',\n",
    "                          sep=' ',header=None).values\n",
    "    print(ecg_temp.shape,f)\n",
    "    ecg = np.zeros((ecg_temp.shape[0],ecg_temp.shape[1]+1))\n",
    "    ecg[:,0],ecg[:,2] = ecg_temp[:,1],ecg_temp[:,0]\n",
    "    ecg_rr_ts,ecg_rr_sam = get_hr(ecg)\n",
    "    if len(ecg_rr_sam)<100:\n",
    "        continue\n",
    "    print(ecg_rr_ts.shape,f)\n",
    "    ecg_rr = np.zeros((len(ecg_rr_ts),2))\n",
    "    ecg_rr[:,0] = ecg_rr_ts\n",
    "    ecg_rr[:,1] = ecg_rr_sam\n",
    "    pickle.dump(ecg_rr,open(f+'ecg_rr.p','wb'))\n",
    "    \n",
    "    if os.path.isfile(f+'ecg_rr.p') and st>0:\n",
    "        ecg_rr = pickle.load(open(f+'ecg_rr.p','rb'))\n",
    "        ecg_rr_baseline = ecg_rr\n",
    "        ecg_rr[:,1] = stats.mstats.winsorize(ecg_rr[:,1],limits=.1)\n",
    "        ecg_windows = get_windows(ecg_rr,window_size=60,offset=30,fs=1)\n",
    "        final_ecg_windows = ecg_windows\n",
    "        ecg_features = np.array([np.array([window[0,0],window[-1,0]]+ecg_feature_computation(window[:,0],window[:,1])) for window in final_ecg_windows])\n",
    "        for i in range(2,13,1):\n",
    "            ecg_features[:,i] = StandardScaler().fit_transform(ecg_features[:,i].reshape(-1,1)).reshape(-1)\n",
    "        print(len(ecg_windows),len(final_ecg_windows),ecg_features.shape,path,f,ecg_features.shape)\n",
    "        pickle.dump(ecg_features,open(f+'features22.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store' 'SI01' 'SI02' 'SI03' 'SI04' 'SI05' 'SI06' 'SI07' 'SI08'\n",
      " 'SI09' 'SI10' 'SI11' 'SI12' 'SI13' 'SI14' 'SI15' 'SI16' 'SI17' 'SI18'\n",
      " 'SI19' 'SI20' 'SI21' 'SI22' 'SI23' 'SI24' '.ipynb_checkpoints'\n",
      " 'feature.csv' 'feature_rip.csv' 'feature_ecg.csv' 'feature_all.csv']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2584, 15)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soujanya Chatterjee\n",
    "\t\n",
    "# 2:06 PM (9 minutes ago)\n",
    "\t\n",
    "# to me\n",
    "import pandas as pd, numpy as np, os, csv, glob, math, matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from datetime import datetime\n",
    "from scipy.stats import *\n",
    "import gzip\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def find_majority(k):\n",
    "    myMap = {}\n",
    "    maximum = ( '', 0 ) # (occurring element, occurrences)\n",
    "    for n in k:\n",
    "        if n in myMap: myMap[n] += 1\n",
    "        else: myMap[n] = 1\n",
    "\n",
    "        # Keep track of maximum on the go\n",
    "        if myMap[n] > maximum[1]: maximum = (n,myMap[n])\n",
    "\n",
    "    return maximum[0]\n",
    "\n",
    "# _dir = 'W:\\\\Students\\\\cstress_features\\\\data\\\\data\\\\SI02\\\\'\n",
    "\n",
    "def decodeLabel(label):\n",
    "    label = label[:2]  # Only the first 2 characters designate the label code\n",
    "\n",
    "    mapping = {'c1': 0, 'c2': 1, 'c3': 1, 'c4': 0, 'c5': 0, 'c6': 0, 'c7': 2}\n",
    "\n",
    "    return mapping[label]\n",
    "\n",
    "def readstressmarks(participantID, filename):\n",
    "    features = []\n",
    "    for file in os.listdir(filename):    \n",
    "        if file.endswith(\"marks.txt.gz\"):        \n",
    "            with gzip.open(os.path.join(filename, file), 'r') as file:\n",
    "                for line in file.readlines():\n",
    "                    line = line.decode('utf8').strip()\n",
    "                    parts = [x.strip() for x in line.split(',')]                    \n",
    "                    label = parts[0][:2]  \n",
    "                    if label not in ['c7','c6']:\n",
    "                        stressClass = decodeLabel(label)\n",
    "                        features.append([participantID, stressClass, int(parts[2]), int(parts[3])])\n",
    "    return np.array(features)\n",
    "\n",
    "_dirr = './data/'\n",
    "parti = np.array(os.listdir(_dirr) )\n",
    "print(parti)\n",
    "header = ['participant','starttime','endtime','label','f_1','f_2','f_3','f_4','f_5','f_6','f_7','f_8','f_9','f_10','f_11']\n",
    "fea_cols = ['f_1','f_2','f_3','f_4','f_5','f_6','f_7','f_8','f_9','f_10','f_11']\n",
    "data = []\n",
    "for p in parti:\n",
    "    if p in ['feature.csv','feature_ecg.csv','feature_rip.csv','SI05','SI09','SI11','SI14','SI23','SI24','.ipynb_checkpoints']:\n",
    "        continue\n",
    "    else:\n",
    "        if os.path.isdir(os.path.join(_dirr,p)):\n",
    "           \n",
    "            _dir = (os.path.join(_dirr,p))\n",
    "            gt_marks = readstressmarks(p,_dir)\n",
    "            groundtruth = pd.DataFrame({'participant': gt_marks[:, 0], 'label': gt_marks[:, 1], 'starttime': gt_marks[:, 2],\n",
    "                                        'endtime': gt_marks[:, 3]}, columns=['participant','label','starttime','endtime'])\n",
    "            groundtruth = groundtruth.sort_values('starttime')\n",
    "   \n",
    "\n",
    "            for file in os.listdir(_dir):    \n",
    "                    if file.endswith(\"22.p\"):                    \n",
    "                        with open(_dir+'/'+file, 'rb') as f:  \n",
    "                            x = pickle.load(f)\n",
    "#             print(x)\n",
    "            dataset = pd.DataFrame({'starttime': x[:, 0], 'endtime': x[:, 1], 'f_1': x[:, 2]\n",
    "                                   , 'f_2': x[:, 3], 'f_3': x[:, 4], 'f_4': x[:, 5]\n",
    "                                   , 'f_5': x[:, 6], 'f_6': x[:, 7], 'f_7': x[:, 8]\n",
    "                                   , 'f_8': x[:, 9], 'f_9': x[:, 10], 'f_10': x[:, 11]\n",
    "                                   , 'f_11': x[:, 12]}, columns=['starttime','endtime','f_1','f_2','f_3','f_4','f_5','f_6','f_7','f_8',\n",
    "                                                                 'f_9','f_10','f_11'])\n",
    "           \n",
    "            dataset = dataset.sort_values('starttime')\n",
    "\n",
    "            for gt in range(len(dataset)):\n",
    "                starttime = int(dataset['starttime'].iloc[gt])\n",
    "                endtime = int(dataset['endtime'].iloc[gt])\n",
    "                result = []\n",
    "                for line in range(len(groundtruth)):\n",
    "                    id, gtt, st, et = [groundtruth['participant'].iloc[line], groundtruth['label'].iloc[line], int(groundtruth['starttime'].iloc[line]),\n",
    "                                      int(groundtruth['endtime'].iloc[line])]\n",
    "                    if starttime < st:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (starttime > st) and (endtime < et):\n",
    "                            result.append(gtt)\n",
    "                        if result:\n",
    "                            fea = list(dataset[fea_cols].iloc[gt])\n",
    "                            inter_data = [p, st,et,find_majority(result)],(fea)\n",
    "                            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "                            data.append(flatten(inter_data))\n",
    "    #         print(data)\n",
    "df = pd.DataFrame(data)\n",
    "df.fillna(df.mean(),inplace=True)\n",
    "df.to_csv(_dirr + '/' + 'feature_ecg.csv', index=False, header=header)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2584, 11) (2584,) 492\n"
     ]
    }
   ],
   "source": [
    "feature_file = './data/feature_ecg.csv'\n",
    "feature = pd.read_csv(feature_file).values\n",
    "y = np.int64(feature[:,3])\n",
    "X = feature[:,4:]\n",
    "print(X.shape,y.shape,np.sum(y))\n",
    "groups = feature[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 18 folds for each of 240 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 48 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done 354 tasks      | elapsed:   14.4s\n",
      "[Parallel(n_jobs=-1)]: Done 552 tasks      | elapsed:   23.6s\n",
      "[Parallel(n_jobs=-1)]: Done 786 tasks      | elapsed:   31.7s\n",
      "[Parallel(n_jobs=-1)]: Done 1056 tasks      | elapsed:   42.0s\n",
      "[Parallel(n_jobs=-1)]: Done 1362 tasks      | elapsed:   54.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1704 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2082 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 2496 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 2946 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 3432 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 3954 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4320 out of 4320 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (CV score=0.916):\n",
      "{'C': 1.2589254117941673, 'class_weight': {0: 0.4, 1: 0.6}, 'gamma': 0.0625, 'kernel': 'rbf', 'probability': True}\n",
      "[[2004   88]\n",
      " [ 130  362]]               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      2092\n",
      "           1       0.80      0.74      0.77       492\n",
      "\n",
      "    accuracy                           0.92      2584\n",
      "   macro avg       0.87      0.85      0.86      2584\n",
      "weighted avg       0.91      0.92      0.91      2584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "paramGrid = {'kernel': ['rbf'],\n",
    "             'C': np.logspace(.1,4,5),\n",
    "             'gamma': [np.power(2,np.float(x)) for x in np.arange(-4, 4, .5)],\n",
    "             'class_weight': [{0: w, 1: 1 - w} for w in [.4,.3,.2]],\n",
    "             'probability':[True]\n",
    "}\n",
    "# clf = Pipeline([('rf', SVC())])\n",
    "clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='f1_weighted',verbose=5)\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups))\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1.2589254117941673, break_ties=False, cache_size=200,\n",
      "    class_weight={0: 0.4, 1: 0.6}, coef0=0.0, decision_function_shape='ovr',\n",
      "    degree=3, gamma=0.0625, kernel='rbf', max_iter=-1, probability=True,\n",
      "    random_state=None, shrinking=True, tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "clf.set_params(probability=True)\n",
    "print(clf)\n",
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/cc3/ecg_model_feature_standardization.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_porter import Porter\n",
    "porter = Porter(clf, language='java')\n",
    "output = porter.export(export_data=True)\n",
    "# print(output)\n",
    "text_file = open(\"SVM1.java\", \"w\")\n",
    "text_file.write(output)\n",
    "text_file.close()\n",
    "print(clf.probA_,clf.probB_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn_porter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7c6cc3c7e285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn_porter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPorter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn_porter'"
     ]
    }
   ],
   "source": [
    "from sklearn_porter import Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
