{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine ECG and RR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 49)\n",
      "(752, 49)\n",
      "(720, 49)\n",
      "(759, 49)\n",
      "(820, 49)\n",
      "(337, 49)\n",
      "(790, 49)\n",
      "(772, 49)\n",
      "(750, 49)\n",
      "(604, 49)\n",
      "(811, 49)\n",
      "(770, 49)\n",
      "(656, 49)\n",
      "(769, 49)\n",
      "(673, 49)\n",
      "(753, 49)\n",
      "(633, 49)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tomkin import detect_rpeak\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from outlier_calculation import Quality,compute_outlier_ecg\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel,delayed\n",
    "from data_quality import ECGQualityCalculation\n",
    "from joblib import delayed,Parallel\n",
    "from copy import deepcopy\n",
    "from ecg import ecg_feature_computation\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,QuantileTransformer\n",
    "import gzip\n",
    "\n",
    "def rip_cycle_feature_computation(peaks_datastream: np.ndarray,\n",
    "                                  valleys_datastream: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Respiration Feature Implementation. The respiration feature values are\n",
    "    derived from the following paper:\n",
    "    'puffMarker: a multi-sensor approach for pinpointing the timing of first lapse in smoking cessation'\n",
    "    Removed due to lack of current use in the implementation\n",
    "    roc_max = []  # 8. ROC_MAX = max(sample[j]-sample[j-1])\n",
    "    roc_min = []  # 9. ROC_MIN = min(sample[j]-sample[j-1])\n",
    "\n",
    "    :param peaks_datastream: list of peak datapoints\n",
    "    :param valleys_datastream: list of valley datapoints\n",
    "    :return: lists of DataPoints each representing a specific feature calculated from the respiration cycle\n",
    "    found from the peak valley inputs\n",
    "    \"\"\"\n",
    "\n",
    "    inspiration_duration = []  # 1 Inhalation duration\n",
    "    expiration_duration = []  # 2 Exhalation duration\n",
    "    respiration_duration = []  # 3 Respiration duration\n",
    "    inspiration_expiration_ratio = []  # 4 Inhalation and Exhalation ratio\n",
    "    stretch = []  # 5 Stretch\n",
    "    upper_stretch = []  # 6. Upper portion of the stretch calculation\n",
    "    lower_stretch = []  # 7. Lower portion of the stretch calculation\n",
    "    delta_previous_inspiration_duration = []  # 10. BD_INSP = INSP(i)-INSP(i-1)\n",
    "    delta_previous_expiration_duration = []  # 11. BD_EXPR = EXPR(i)-EXPR(i-1)\n",
    "    delta_previous_respiration_duration = []  # 12. BD_RESP = RESP(i)-RESP(i-1)\n",
    "    delta_previous_stretch_duration = []  # 14. BD_Stretch= Stretch(i)-Stretch(i-1)\n",
    "    delta_next_inspiration_duration = []  # 19. FD_INSP = INSP(i)-INSP(i+1)\n",
    "    delta_next_expiration_duration = []  # 20. FD_EXPR = EXPR(i)-EXPR(i+1)\n",
    "    delta_next_respiration_duration = []  # 21. FD_RESP = RESP(i)-RESP(i+1)\n",
    "    delta_next_stretch_duration = []  # 23. FD_Stretch= Stretch(i)-Stretch(i+1)\n",
    "    neighbor_ratio_expiration_duration = []  # 29. D5_EXPR(i) = EXPR(i) / avg(EXPR(i-2)...EXPR(i+2))\n",
    "    neighbor_ratio_stretch_duration = []  # 32. D5_Stretch = Stretch(i) / avg(Stretch(i-2)...Stretch(i+2))\n",
    "\n",
    "    valleys = valleys_datastream\n",
    "    peaks = peaks_datastream[:-1]\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "\n",
    "        delta = peak[0] - valleys[i][0]\n",
    "        inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - peak[0]\n",
    "        expiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - valley_start_time\n",
    "        respiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        ratio = (peak[0] - valley_start_time) / (valleys[i + 1][0] - peak[0])\n",
    "        inspiration_expiration_ratio.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        value = peak[1] - valleys[i + 1][1]\n",
    "        stretch.append(np.array([valley_start_time,valley_end_time,value]))\n",
    "\n",
    "    for i, point in enumerate(inspiration_duration):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "        if i == 0:  # Edge case\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i - 1][2]\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i - 1][2]\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i - 1][2]\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i - 1][2]\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        if i == len(inspiration_duration) - 1:\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i + 1][2]\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i + 1][2]\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i + 1][2]\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i + 1][2]\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        stretch_average = 0\n",
    "        expiration_average = 0\n",
    "        count = 0.0\n",
    "        for j in [-2, -1, 1, 2]:\n",
    "            if i + j < 0 or i + j >= len(inspiration_duration):\n",
    "                continue\n",
    "            stretch_average += stretch[i + j][2]\n",
    "            expiration_average += expiration_duration[i + j][2]\n",
    "            count += 1\n",
    "\n",
    "        stretch_average /= count\n",
    "        expiration_average /= count\n",
    "\n",
    "        ratio = stretch[i][2] / stretch_average\n",
    "        neighbor_ratio_stretch_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        ratio = expiration_duration[i][2] / expiration_average\n",
    "        neighbor_ratio_expiration_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "    # Begin assembling datastream for output\n",
    "    inspiration_duration_datastream = np.array(inspiration_duration)[1:-1]\n",
    "\n",
    "    expiration_duration_datastream = np.array(expiration_duration)[1:-1]\n",
    "\n",
    "    respiration_duration_datastream = np.array(respiration_duration)[1:-1]\n",
    "\n",
    "    inspiration_expiration_ratio_datastream = np.array(inspiration_expiration_ratio)[1:-1]\n",
    "\n",
    "    stretch_datastream = np.array(stretch)[1:-1]\n",
    "\n",
    "    delta_previous_inspiration_duration_datastream = np.array(delta_previous_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_expiration_duration_datastream = np.array(delta_previous_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_respiration_duration_datastream = np.array(delta_previous_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_stretch_duration_datastream = np.array(delta_previous_stretch_duration)[1:-1]\n",
    "\n",
    "    delta_next_inspiration_duration_datastream = np.array(delta_next_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_expiration_duration_datastream = np.array(delta_next_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_respiration_duration_datastream = np.array(delta_next_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_stretch_duration_datastream = np.array(delta_next_stretch_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_expiration_datastream = np.array(neighbor_ratio_expiration_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_stretch_datastream = np.array(neighbor_ratio_stretch_duration)[1:-1]\n",
    "\n",
    "    return np.concatenate([inspiration_duration_datastream,\n",
    "                           expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           inspiration_expiration_ratio_datastream[:,2].reshape(-1,1),\n",
    "                           stretch_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_expiration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_stretch_datastream[:,2].reshape(-1,1)],axis=1)\n",
    "\n",
    "def get_windows(data,window_size=10,offset=10,fs=1):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>t-window_size*1000/2)&(data[:,0]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<30:\n",
    "            continue\n",
    "        window_col.append(data[index,:])\n",
    "    return window_col\n",
    "\n",
    "def get_std_chest(window,start=1,end=4):\n",
    "    return np.array([np.mean(window[:,0]),np.sqrt(np.sum(np.power(np.std(window[:,start:end],axis=0),2)))])\n",
    "\n",
    "\n",
    "def filter_ecg_windows(ecg_windows,acl_std):\n",
    "    final_ecg_windows = []\n",
    "    for window in ecg_windows:\n",
    "        index = np.where((acl_std[:,0]>window[0,0])&(acl_std[:,0]<window[-1,0]))[0]\n",
    "        if len(index)==0:\n",
    "            continue\n",
    "        window_temp = acl_std[index,1].reshape(-1)\n",
    "        if len(window_temp[window_temp>.21])/len(window_temp) > .5:\n",
    "            continue\n",
    "        final_ecg_windows.append(window)\n",
    "    return final_ecg_windows\n",
    "\n",
    "def get_rip_windows(data,window_size=60,offset=10,fs=.2):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>=t-window_size*1000/2)&(data[:,1]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<10:\n",
    "            continue\n",
    "        window_col.append(data[index,:])\n",
    "    return window_col\n",
    "\n",
    "def get_all_windows(data,rip_data,window_size=60,offset=10,fs=.2):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>=t-window_size*1000/2)&(data[:,0]<=t+window_size*1000/2))[0]\n",
    "        index_rip = np.where((rip_data[:,0]>=t-window_size*1000/2)&(rip_data[:,1]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<30 or len(index_rip)<10:\n",
    "            continue\n",
    "        window_col.append([data[index,:],rip_data[index_rip,:]])\n",
    "    return window_col\n",
    "def get_features(a):\n",
    "    window = a[0]\n",
    "    ecg_feature = ecg_feature_computation(window[:,0],window[:,1])\n",
    "    window = a[1]\n",
    "    rip_feature = list(np.mean(window[:,2:],axis=0))+list(np.std(window[:,2:],axis=0))+ \\\n",
    "    list(np.percentile(window[:,2:],80,axis=0))+list(np.percentile(window[:,2:],20,axis=0))\n",
    "    feature = list([window[0,0],window[-1,0]])+ecg_feature+rip_feature\n",
    "    return np.array(feature)\n",
    "    \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "path = './data/'\n",
    "participants = [path + f +'/' for f in os.listdir(path) if f[0]=='S']\n",
    "for f in participants:\n",
    "    if 'ecg.txt.gz' not in os.listdir(f):\n",
    "        continue\n",
    "    if os.path.isfile(f+'ecg_rr.p') and os.path.isfile(f+'pv.p') :\n",
    "        ecg_rr = pickle.load(open(f+'ecg_rr.p','rb'))\n",
    "        ecg_rr_baseline = ecg_rr\n",
    "        from scipy import stats\n",
    "        ecg_rr[:,1] = stats.mstats.winsorize(ecg_rr[:,1],limits=.01)\n",
    "        \n",
    "        peaks,valleys = pickle.load(open(f+'pv.p','rb'))\n",
    "        rip_features = rip_cycle_feature_computation(peaks,valleys)\n",
    "        rip_features = rip_features[:,np.array([0,1,2,3,4,5,6,7,8,-2,-1])]\n",
    "        for c in range(2,rip_features.shape[1]):\n",
    "            rip_features[:,c] = stats.mstats.winsorize(rip_features[:,c],limits=.01)\n",
    "            \n",
    "        all_windows = get_all_windows(ecg_rr,rip_features,window_size=60,offset=10,fs=1)\n",
    "        all_features = np.array(list(map(lambda a:get_features(a),all_windows)))\n",
    "        for i in range(2,all_features.shape[1],1):\n",
    "            all_features[:,i] = StandardScaler().fit_transform(all_features[:,i].reshape(-1,1)).reshape(-1)\n",
    "        pickle.dump(all_features,open(f+'features_rip_ecg.p','wb'))\n",
    "        print(all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "SI01\n",
      "SI02\n",
      "SI03\n",
      "SI04\n",
      "SI05\n",
      "SI06\n",
      "SI07\n",
      "SI08\n",
      "SI09\n",
      "SI10\n",
      "SI11\n",
      "SI12\n",
      "SI13\n",
      "SI14\n",
      "SI15\n",
      "SI16\n",
      "SI17\n",
      "SI18\n",
      "SI19\n",
      "SI20\n",
      "SI21\n",
      "SI22\n",
      "SI23\n",
      "SI24\n",
      ".ipynb_checkpoints\n",
      "feature.csv\n",
      "feature_rip.csv\n",
      "feature_ecg.csv\n",
      "feature_all.csv\n",
      "feature_rip_ecg.csv\n",
      "feature_ecg_norm.csv\n",
      "(6926, 51)\n"
     ]
    }
   ],
   "source": [
    "# Soujanya Chatterjee\n",
    "\t\n",
    "# 2:06 PM (9 minutes ago)\n",
    "\t\n",
    "# to me\n",
    "import pandas as pd, numpy as np, os, csv, glob, math, matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from datetime import datetime\n",
    "from scipy.stats import *\n",
    "import gzip\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def find_majority(k):\n",
    "    myMap = {}\n",
    "    maximum = ( '', 0 ) # (occurring element, occurrences)\n",
    "    for n in k:\n",
    "        if n in myMap: myMap[n] += 1\n",
    "        else: myMap[n] = 1\n",
    "\n",
    "        # Keep track of maximum on the go\n",
    "        if myMap[n] > maximum[1]: maximum = (n,myMap[n])\n",
    "\n",
    "    return maximum[0]\n",
    "\n",
    "# _dir = 'W:\\\\Students\\\\cstress_features\\\\data\\\\data\\\\SI02\\\\'\n",
    "\n",
    "def decodeLabel(label):\n",
    "    label = label[:2]  # Only the first 2 characters designate the label code\n",
    "\n",
    "    mapping = {'c1': 0, 'c2': 1, 'c3': 1, 'c4': 0, 'c5': 0, 'c6': 0, 'c7': 2}\n",
    "\n",
    "    return mapping[label]\n",
    "\n",
    "def readstressmarks(participantID, filename):\n",
    "    features = []\n",
    "    for file in os.listdir(filename):    \n",
    "        if file.endswith(\"marks.txt.gz\"):        \n",
    "            with gzip.open(os.path.join(filename, file), 'r') as file:\n",
    "                for line in file.readlines():\n",
    "                    line = line.decode('utf8').strip()\n",
    "                    parts = [x.strip() for x in line.split(',')]                    \n",
    "                    label = parts[0][:2]  \n",
    "                    if label not in ['c7','c6']:\n",
    "                        stressClass = decodeLabel(label)\n",
    "                        features.append([participantID, stressClass, int(parts[2]), int(parts[3])])\n",
    "    return np.array(features)\n",
    "\n",
    "_dirr = './data/'\n",
    "parti = np.array(os.listdir(_dirr) )\n",
    "header = ['participant','starttime','endtime','label'] + ['f_'+str(i) for i in range(47)]\n",
    "fea_cols = ['f_'+str(i) for i in range(47)]\n",
    "data = []\n",
    "for p in parti:\n",
    "    print(p)\n",
    "    if p in ['feature.csv','feature_ecg.csv','feature_rip.csv','feature_all.csv',\n",
    "             '.ipynb_checkpoints']:\n",
    "        continue\n",
    "    else:\n",
    "        if os.path.isdir(os.path.join(_dirr,p)):\n",
    "            _dir = (os.path.join(_dirr,p))\n",
    "            gt_marks = readstressmarks(p,_dir)\n",
    "            if len(gt_marks)==0:\n",
    "                continue\n",
    "            groundtruth = pd.DataFrame({'participant': gt_marks[:, 0], 'label': gt_marks[:, 1], 'starttime': gt_marks[:, 2],\n",
    "                                        'endtime': gt_marks[:, 3]}, columns=['participant','label','starttime','endtime'])\n",
    "            groundtruth = groundtruth.sort_values('starttime')\n",
    "   \n",
    "            check = False\n",
    "            for file in os.listdir(_dir):    \n",
    "                    if file.endswith('features_rip_ecg.p'):                    \n",
    "                        with open(_dir+'/'+file, 'rb') as f:  \n",
    "                            x = pickle.load(f)\n",
    "                            check  =True\n",
    "            if not check:\n",
    "                continue\n",
    "#             print(x.shape)\n",
    "#             dataset = pd.DataFrame({'starttime': x[:, 0], 'endtime': x[:, 1], 'f_1': x[:, 2]\n",
    "#                                    , 'f_2': x[:, 3], 'f_3': x[:, 4], 'f_4': x[:, 5]\n",
    "#                                    , 'f_5': x[:, 6], 'f_6': x[:, 7], 'f_7': x[:, 8]\n",
    "#                                    , 'f_8': x[:, 9], 'f_9': x[:, 10], 'f_10': x[:, 11]\n",
    "#                                    , 'f_11': x[:, 12]}, columns=['starttime','endtime','f_1','f_2','f_3','f_4','f_5','f_6','f_7','f_8',\n",
    "#                                                                  'f_9','f_10','f_11'])\n",
    "            \n",
    "            dataset = pd.DataFrame(x,columns=['starttime','endtime']+['f_'+str(i) for i in range(x.shape[1]-2)])\n",
    "            dataset = dataset.sort_values('starttime')\n",
    "\n",
    "            for gt in range(len(dataset)):\n",
    "                starttime = int(dataset['starttime'].iloc[gt])\n",
    "                endtime = int(dataset['endtime'].iloc[gt])\n",
    "                result = []\n",
    "                for line in range(len(groundtruth)):\n",
    "                    id, gtt, st, et = [groundtruth['participant'].iloc[line], groundtruth['label'].iloc[line], int(groundtruth['starttime'].iloc[line]),\n",
    "                                      int(groundtruth['endtime'].iloc[line])]\n",
    "                    if starttime < st:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (starttime > st) and (endtime < et):\n",
    "                            result.append(gtt)\n",
    "                        if result:\n",
    "                            fea = list(dataset[fea_cols].iloc[gt])\n",
    "                            inter_data = [p, st,et,find_majority(result)],(fea)\n",
    "                            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "                            data.append(flatten(inter_data))\n",
    "    #         print(data)\n",
    "df = pd.DataFrame(data)\n",
    "df.fillna(df.mean(),inplace=True)\n",
    "print(df.shape)\n",
    "df.to_csv(_dirr + '/' + 'feature_rip_ecg.csv', index=False, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6926, 11) (6926,) 1347 [0 1] 17\n"
     ]
    }
   ],
   "source": [
    "feature_file = './data/feature_rip_ecg.csv'\n",
    "feature = pd.read_csv(feature_file).values\n",
    "y = np.int64(feature[:,3])\n",
    "X = feature[:,4:4+11]\n",
    "groups = feature[:,0]\n",
    "print(X.shape,y.shape,np.sum(y),np.unique(y),len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 17 folds for each of 200 candidates, totalling 3400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done 120 tasks      | elapsed: 20.0min\n",
      "[Parallel(n_jobs=40)]: Done 200 out of 200 | elapsed: 30.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModifiedGridSearchCV(cv=[(array([   0,    1,    2, ..., 6923, 6924, 6925]),\n",
       "                          array([1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718,\n",
       "       1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729,\n",
       "       1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740,\n",
       "       1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751,\n",
       "       1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762,\n",
       "       1763, 176...\n",
       "       6.15848211e-06, 5.45559478e-05, 4.83293024e-04, 4.28133240e-03,\n",
       "       3.79269019e-02, 3.35981829e-01, 2.97635144e+00, 2.63665090e+01,\n",
       "       2.33572147e+02, 2.06913808e+03, 1.83298071e+04, 1.62377674e+05,\n",
       "       1.43844989e+06, 1.27427499e+07, 1.12883789e+08, 1.00000000e+09]),\n",
       "                                 'kernel': ['rbf'], 'probability': [True],\n",
       "                                 'verbose': [False]},\n",
       "                     pre_dispatch='2*n_jobs', refit=True,\n",
       "                     scoring=<function f1Bias_scorer_CV at 0x7fab9589ad08>,\n",
       "                     verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba',n_jobs=-1)[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator=estimator, param_grid=param_grid, scoring=scoring,  n_jobs=n_jobs, iid=iid,\n",
    "                refit=refit, cv=cv, verbose=verbose, pre_dispatch=pre_dispatch, error_score=error_score)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "        cv = self.cv\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "        if self.verbose > 0:\n",
    "#             if isinstance(parameter_iterable, Sized):\n",
    "            n_candidates = len(parameter_iterable)\n",
    "            print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "                  \" {2} fits\".format(len(cv), n_candidates,\n",
    "                                     n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self\n",
    "\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "X1 = StandardScaler().fit_transform(X)\n",
    "delta = 0.1\n",
    "parameters1 = {'kernel': ['rbf'],\n",
    "              'C': np.logspace(0,2,2),\n",
    "              'gamma': np.logspace(-9,9,20),\n",
    "              'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, .50, delta)],\n",
    "              'probability':[True],\n",
    "              'verbose':[False],\n",
    "              'cache_size':[2000]}\n",
    "# parameters = {\n",
    "#     'min_samples_leaf': [4],\n",
    "#     'max_features': [.7,1],\n",
    "#     'n_estimators': [100,200,300],\n",
    "#     'n_jobs': [-1],\n",
    "#     'criterion':['gini','entropy'],\n",
    "#     'class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, 1.0, delta)],\n",
    "#     'random_state': [42]\n",
    "#        }\n",
    "svc = SVC()\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X1,y,groups=groups)),\n",
    "                                   n_jobs=40, scoring=f1Bias_scorer_CV, verbose=1, iid=False)\n",
    "grid_search.fit(X1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7781225756400311 0.4587211070230058\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      5579\n",
      "           1       0.81      0.74      0.78      1347\n",
      "\n",
      "    accuracy                           0.92      6926\n",
      "   macro avg       0.88      0.85      0.86      6926\n",
      "weighted avg       0.92      0.92      0.92      6926\n",
      "\n",
      "[[5351  228]\n",
      " [ 344 1003]]\n",
      "0.7845303867403315\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "clf\n",
    "\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X1, y, gkf.split(X1,y,groups=groups))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(CV_probs,50)\n",
    "plt.show()\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6926, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)\n",
    "clf.fit(X1,y)\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/apply_cstress_to_rice/models/ecg_model.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "paramGrid = {\n",
    "             'pca__n_components':[4,5,6],\n",
    "             'rf__kernel': ['rbf'],\n",
    "             'rf__C': np.logspace(.1,4,3),\n",
    "             'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-4, 0, .5)],\n",
    "             'rf__class_weight': [{0: w, 1: 1 - w} for w in [.4,.3,.2]],\n",
    "             'rf__probability':[True]\n",
    "}\n",
    "\n",
    "paramGrid = {\n",
    "             'kernel': ['rbf'],\n",
    "             'C': np.logspace(.1,4,3),\n",
    "             'gamma': [np.power(2,np.float(x)) for x in np.arange(-4, 0, .5)],\n",
    "             'class_weight': [{0: w, 1: 1 - w} for w in [.4,.3,.2]],\n",
    "             'probability':[True]\n",
    "}\n",
    "# clf = Pipeline([('pca',PCA()),('rf', SVC())])\n",
    "clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='f1_weighted',verbose=5)\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups))\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(clf)\n",
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/cc3/ecg_rip_model_feature_standardization_minnesota.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_porter import Porter\n",
    "porter = Porter(clf, language='java')\n",
    "output = porter.export(export_data=True)\n",
    "# print(output)\n",
    "text_file = open(\"SVM.java\", \"w\")\n",
    "text_file.write(output)\n",
    "text_file.close()\n",
    "print(clf.probA_,clf.probB_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "High Performance CC3.3",
   "language": "python",
   "name": "cc33_high_performance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
