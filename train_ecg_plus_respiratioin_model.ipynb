{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine ECG and RR features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tomkin import detect_rpeak\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from outlier_calculation import Quality,compute_outlier_ecg\n",
    "# from hrvanalysis import remove_ectopic_beats\n",
    "from joblib import Parallel,delayed\n",
    "from data_quality import ECGQualityCalculation\n",
    "from joblib import delayed,Parallel\n",
    "from copy import deepcopy\n",
    "from ecg import ecg_feature_computation\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,QuantileTransformer\n",
    "import gzip\n",
    "\n",
    "def rip_cycle_feature_computation(peaks_datastream: np.ndarray,\n",
    "                                  valleys_datastream: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Respiration Feature Implementation. The respiration feature values are\n",
    "    derived from the following paper:\n",
    "    'puffMarker: a multi-sensor approach for pinpointing the timing of first lapse in smoking cessation'\n",
    "    Removed due to lack of current use in the implementation\n",
    "    roc_max = []  # 8. ROC_MAX = max(sample[j]-sample[j-1])\n",
    "    roc_min = []  # 9. ROC_MIN = min(sample[j]-sample[j-1])\n",
    "\n",
    "    :param peaks_datastream: list of peak datapoints\n",
    "    :param valleys_datastream: list of valley datapoints\n",
    "    :return: lists of DataPoints each representing a specific feature calculated from the respiration cycle\n",
    "    found from the peak valley inputs\n",
    "    \"\"\"\n",
    "\n",
    "    inspiration_duration = []  # 1 Inhalation duration\n",
    "    expiration_duration = []  # 2 Exhalation duration\n",
    "    respiration_duration = []  # 3 Respiration duration\n",
    "    inspiration_expiration_ratio = []  # 4 Inhalation and Exhalation ratio\n",
    "    stretch = []  # 5 Stretch\n",
    "    upper_stretch = []  # 6. Upper portion of the stretch calculation\n",
    "    lower_stretch = []  # 7. Lower portion of the stretch calculation\n",
    "    delta_previous_inspiration_duration = []  # 10. BD_INSP = INSP(i)-INSP(i-1)\n",
    "    delta_previous_expiration_duration = []  # 11. BD_EXPR = EXPR(i)-EXPR(i-1)\n",
    "    delta_previous_respiration_duration = []  # 12. BD_RESP = RESP(i)-RESP(i-1)\n",
    "    delta_previous_stretch_duration = []  # 14. BD_Stretch= Stretch(i)-Stretch(i-1)\n",
    "    delta_next_inspiration_duration = []  # 19. FD_INSP = INSP(i)-INSP(i+1)\n",
    "    delta_next_expiration_duration = []  # 20. FD_EXPR = EXPR(i)-EXPR(i+1)\n",
    "    delta_next_respiration_duration = []  # 21. FD_RESP = RESP(i)-RESP(i+1)\n",
    "    delta_next_stretch_duration = []  # 23. FD_Stretch= Stretch(i)-Stretch(i+1)\n",
    "    neighbor_ratio_expiration_duration = []  # 29. D5_EXPR(i) = EXPR(i) / avg(EXPR(i-2)...EXPR(i+2))\n",
    "    neighbor_ratio_stretch_duration = []  # 32. D5_Stretch = Stretch(i) / avg(Stretch(i-2)...Stretch(i+2))\n",
    "\n",
    "    valleys = valleys_datastream\n",
    "    peaks = peaks_datastream[:-1]\n",
    "\n",
    "    for i, peak in enumerate(peaks):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "\n",
    "        delta = peak[0] - valleys[i][0]\n",
    "        inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - peak[0]\n",
    "        expiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        delta = valleys[i + 1][0] - valley_start_time\n",
    "        respiration_duration.append(np.array([valley_start_time,valley_end_time,delta/1000]))\n",
    "\n",
    "        ratio = (peak[0] - valley_start_time) / (valleys[i + 1][0] - peak[0])\n",
    "        inspiration_expiration_ratio.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        value = peak[1] - valleys[i + 1][1]\n",
    "        stretch.append(np.array([valley_start_time,valley_end_time,value]))\n",
    "\n",
    "    for i, point in enumerate(inspiration_duration):\n",
    "        valley_start_time = valleys[i][0]\n",
    "        valley_end_time = valleys[i + 1][0]\n",
    "        if i == 0:  # Edge case\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i - 1][2]\n",
    "            delta_previous_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i - 1][2]\n",
    "            delta_previous_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i - 1][2]\n",
    "            delta_previous_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i - 1][2]\n",
    "            delta_previous_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        if i == len(inspiration_duration) - 1:\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,0]))\n",
    "        else:\n",
    "            delta = inspiration_duration[i][2] - inspiration_duration[i + 1][2]\n",
    "            delta_next_inspiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = expiration_duration[i][2] - expiration_duration[i + 1][2]\n",
    "            delta_next_expiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = respiration_duration[i][2] - respiration_duration[i + 1][2]\n",
    "            delta_next_respiration_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "            delta = stretch[i][2] - stretch[i + 1][2]\n",
    "            delta_next_stretch_duration.append(np.array([valley_start_time,valley_end_time,delta]))\n",
    "\n",
    "        stretch_average = 0\n",
    "        expiration_average = 0\n",
    "        count = 0.0\n",
    "        for j in [-2, -1, 1, 2]:\n",
    "            if i + j < 0 or i + j >= len(inspiration_duration):\n",
    "                continue\n",
    "            stretch_average += stretch[i + j][2]\n",
    "            expiration_average += expiration_duration[i + j][2]\n",
    "            count += 1\n",
    "\n",
    "        stretch_average /= count\n",
    "        expiration_average /= count\n",
    "\n",
    "        ratio = stretch[i][2] / stretch_average\n",
    "        neighbor_ratio_stretch_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "        ratio = expiration_duration[i][2] / expiration_average\n",
    "        neighbor_ratio_expiration_duration.append(np.array([valley_start_time,valley_end_time,ratio]))\n",
    "\n",
    "    # Begin assembling datastream for output\n",
    "    inspiration_duration_datastream = np.array(inspiration_duration)[1:-1]\n",
    "\n",
    "    expiration_duration_datastream = np.array(expiration_duration)[1:-1]\n",
    "\n",
    "    respiration_duration_datastream = np.array(respiration_duration)[1:-1]\n",
    "\n",
    "    inspiration_expiration_ratio_datastream = np.array(inspiration_expiration_ratio)[1:-1]\n",
    "\n",
    "    stretch_datastream = np.array(stretch)[1:-1]\n",
    "\n",
    "    delta_previous_inspiration_duration_datastream = np.array(delta_previous_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_expiration_duration_datastream = np.array(delta_previous_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_respiration_duration_datastream = np.array(delta_previous_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_previous_stretch_duration_datastream = np.array(delta_previous_stretch_duration)[1:-1]\n",
    "\n",
    "    delta_next_inspiration_duration_datastream = np.array(delta_next_inspiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_expiration_duration_datastream = np.array(delta_next_expiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_respiration_duration_datastream = np.array(delta_next_respiration_duration)[1:-1]\n",
    "\n",
    "    delta_next_stretch_duration_datastream = np.array(delta_next_stretch_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_expiration_datastream = np.array(neighbor_ratio_expiration_duration)[1:-1]\n",
    "\n",
    "    neighbor_ratio_stretch_datastream = np.array(neighbor_ratio_stretch_duration)[1:-1]\n",
    "\n",
    "    return np.concatenate([inspiration_duration_datastream,\n",
    "                           expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           inspiration_expiration_ratio_datastream[:,2].reshape(-1,1),\n",
    "                           stretch_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_previous_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_inspiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_expiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_respiration_duration_datastream[:,2].reshape(-1,1),\n",
    "                           delta_next_stretch_duration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_expiration_datastream[:,2].reshape(-1,1),\n",
    "                           neighbor_ratio_stretch_datastream[:,2].reshape(-1,1)],axis=1)\n",
    "\n",
    "def get_windows(data,window_size=10,offset=10,fs=1):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>t-window_size*1000/2)&(data[:,0]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<30:\n",
    "            continue\n",
    "        window_col.append(data[index,:])\n",
    "    return window_col\n",
    "\n",
    "def get_std_chest(window,start=1,end=4):\n",
    "    return np.array([np.mean(window[:,0]),np.sqrt(np.sum(np.power(np.std(window[:,start:end],axis=0),2)))])\n",
    "\n",
    "\n",
    "def filter_ecg_windows(ecg_windows,acl_std):\n",
    "    final_ecg_windows = []\n",
    "    for window in ecg_windows:\n",
    "        index = np.where((acl_std[:,0]>window[0,0])&(acl_std[:,0]<window[-1,0]))[0]\n",
    "        if len(index)==0:\n",
    "            continue\n",
    "        window_temp = acl_std[index,1].reshape(-1)\n",
    "        if len(window_temp[window_temp>.21])/len(window_temp) > .5:\n",
    "            continue\n",
    "        final_ecg_windows.append(window)\n",
    "    return final_ecg_windows\n",
    "\n",
    "def get_rip_windows(data,window_size=60,offset=10,fs=.2):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>=t-window_size*1000/2)&(data[:,1]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<10:\n",
    "            continue\n",
    "        window_col.append(data[index,:])\n",
    "    return window_col\n",
    "\n",
    "def get_all_windows(data,rip_data,window_size=60,offset=10,fs=.2):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>=t-window_size*1000/2)&(data[:,0]<=t+window_size*1000/2))[0]\n",
    "        index_rip = np.where((rip_data[:,0]>=t-window_size*1000/2)&(rip_data[:,1]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<30 or len(index_rip)<10:\n",
    "            continue\n",
    "        window_col.append([data[index,:],rip_data[index_rip,:]])\n",
    "    return window_col\n",
    "\n",
    "# Copyright (c) 2017, MD2K Center of Excellence\n",
    "# All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "#\n",
    "# * Redistributions of source code must retain the above copyright notice, this\n",
    "# list of conditions and the following disclaimer.\n",
    "#\n",
    "# * Redistributions in binary form must reproduce the above copyright notice,\n",
    "# this list of conditions and the following disclaimer in the documentation\n",
    "# and/or other materials provided with the distribution.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "import datetime\n",
    "from scipy.stats import iqr\n",
    "import numpy as np\n",
    "from scipy.stats import iqr\n",
    "from scipy import interpolate, signal\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import matplotlib.patches as mpatches\n",
    "from collections import OrderedDict\n",
    "\n",
    "def frequencyDomain(tmStamps,RRints, lf_bw = 0.11, hf_bw = 0.1):\n",
    "    \n",
    "    #Remove ectopic beats\n",
    "    #RR intervals differing by more than 20% from the one proceeding it are removed\n",
    "    NNs = []\n",
    "    tss = []\n",
    "    for c, rr in enumerate(RRints):        \n",
    "#         if abs(rr - RRints[c-1]) <= 0.20 * RRints[c-1]:\n",
    "        NNs.append(np.int64(rr*1000))\n",
    "        tss.append(tmStamps[c])\n",
    "            \n",
    "            \n",
    "    frequency_range = np.linspace(0.001, 1, 10000)\n",
    "    NNs = np.array(NNs)\n",
    "    NNs = NNs - np.mean(NNs)\n",
    "    result = signal.lombscargle(tss, NNs, frequency_range)\n",
    "        \n",
    "    #Pwelch w/ zero pad     \n",
    "    fxx = frequency_range \n",
    "    pxx = result \n",
    "    \n",
    "    vlf= (0.003, 0.04)\n",
    "    lf = (0.04, 0.15)\n",
    "    hf = (0.15, 0.4)\n",
    "    \n",
    "    plot_labels = ['VLF', 'LF', 'HF']\n",
    "        \n",
    "    df = fxx[1] - fxx[0]\n",
    "    vlf_power = np.trapz(pxx[np.logical_and(fxx >= vlf[0], fxx < vlf[1])], dx = df)      \n",
    "    lf_power = np.trapz(pxx[np.logical_and(fxx >= lf[0], fxx < lf[1])], dx = df)            \n",
    "    hf_power = np.trapz(pxx[np.logical_and(fxx >= hf[0], fxx < hf[1])], dx = df)             \n",
    "    totalPower = vlf_power + lf_power + hf_power\n",
    "    \n",
    "    #Normalize and take log\n",
    "    vlf_NU_log = np.log((vlf_power / (totalPower - vlf_power)) + 1)\n",
    "    lf_NU_log = np.log((lf_power / (totalPower - vlf_power)) + 1)\n",
    "    hf_NU_log = np.log((hf_power / (totalPower - vlf_power)) + 1)\n",
    "    lfhfRation_log = np.log((lf_power / hf_power) + 1)   \n",
    "    \n",
    "    freqDomainFeats = {'VLF_Power': vlf_NU_log, 'LF_Power': lf_NU_log,\n",
    "                       'HF_Power': hf_NU_log, 'LF/HF': lfhfRation_log}\n",
    "                       \n",
    "    return freqDomainFeats\n",
    "\n",
    "\n",
    "\n",
    "def ecg_feature_computation(b,a):\n",
    "    b = b - np.min(b)\n",
    "    return [np.var(a),iqr(a),np.mean(a),np.median(a),np.percentile(a,80),\n",
    "            np.percentile(a,20),np.median(a)]+list(frequencyDomain(b/1000\n",
    "                                                                   ,a/1000).values())\n",
    "\n",
    "\n",
    "\n",
    "def get_features(a):\n",
    "#     try:\n",
    "    window = a[0]\n",
    "    ecg_feature = ecg_feature_computation(window[:,0],window[:,1])\n",
    "    window = a[1]\n",
    "    rip_feature = list(np.mean(window[:,2:],axis=0))+list(np.std(window[:,2:],axis=0))+ \\\n",
    "    list(np.percentile(window[:,2:],80,axis=0))+list(np.percentile(window[:,2:],20,axis=0))\n",
    "    feature = list([window[0,0],window[-1,0]])+ecg_feature+rip_feature\n",
    "    return np.array(feature)\n",
    "#     except:\n",
    "#         window = a[1]\n",
    "#         feature = list([window[0,0],window[-1,0]])+[0]*47\n",
    "#         return np.array(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 49)\n",
      "(752, 49)\n",
      "(720, 49)\n",
      "(759, 49)\n",
      "(820, 49)\n",
      "(337, 49)\n",
      "(790, 49)\n",
      "(772, 49)\n",
      "(750, 49)\n",
      "(604, 49)\n",
      "(811, 49)\n",
      "(770, 49)\n",
      "(656, 49)\n",
      "(769, 49)\n",
      "(673, 49)\n",
      "(753, 49)\n",
      "(633, 49)\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "    \n",
    "path = './data/'\n",
    "participants = [path + f +'/' for f in os.listdir(path) if f[0]=='S']\n",
    "for f in participants:\n",
    "    if 'ecg.txt.gz' not in os.listdir(f):\n",
    "        continue\n",
    "    if os.path.isfile(f+'ecg_rr.p') and os.path.isfile(f+'pv.p') :\n",
    "        ecg_rr = pickle.load(open(f+'ecg_rr.p','rb'))\n",
    "        ecg_rr_baseline = ecg_rr\n",
    "        from scipy import stats\n",
    "        ecg_rr[:,1] = stats.mstats.winsorize(ecg_rr[:,1],limits=.01)\n",
    "        ecg_rr[:,1] = StandardScaler().fit_transform(ecg_rr[:,1].reshape(-1,1)).reshape(-1)\n",
    "        ecg_rr[ecg_rr[:,1]>5,1] = 5\n",
    "        ecg_rr[ecg_rr[:,1]<-5,1] = -5        \n",
    "        peaks,valleys = pickle.load(open(f+'pv.p','rb'))\n",
    "        rip_features = rip_cycle_feature_computation(peaks,valleys)\n",
    "        rip_features = rip_features[:,np.array([0,1,2,3,4,5,6,7,8,-2,-1])]\n",
    "        for c in range(2,rip_features.shape[1]):\n",
    "            rip_features[:,c] = stats.mstats.winsorize(rip_features[:,c],limits=.01)\n",
    "            rip_features[:,c] = StandardScaler().fit_transform(rip_features[:,c].reshape(-1,1)).reshape(-1)\n",
    "            rip_features[rip_features[:,c]>5,c] = 5\n",
    "            rip_features[rip_features[:,c]<-5,c] = -5\n",
    "            \n",
    "        all_windows = get_all_windows(ecg_rr,rip_features,window_size=60,offset=10,fs=1)\n",
    "        all_features = np.array(list(map(lambda a:get_features(a),all_windows)))\n",
    "        for i in range(2,all_features.shape[1],1):\n",
    "            all_features[:,i] = StandardScaler().fit_transform(all_features[:,i].reshape(-1,1)).reshape(-1)\n",
    "#         sum_array = np.array([np.sum(a[2:]) for a in all_features])\n",
    "#         all_features = all_features[sum_array>0,:]\n",
    "        pickle.dump(all_features,open(f+'features_rip_ecg.p','wb'))\n",
    "        print(all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "SI01\n",
      "SI02\n",
      "SI03\n",
      "SI04\n",
      "SI05\n",
      "SI06\n",
      "SI07\n",
      "SI08\n",
      "SI09\n",
      "SI10\n",
      "SI11\n",
      "SI12\n",
      "SI13\n",
      "SI14\n",
      "SI15\n",
      "SI16\n",
      "SI17\n",
      "SI18\n",
      "SI19\n",
      "SI20\n",
      "SI21\n",
      "SI22\n",
      "SI23\n",
      "SI24\n",
      ".ipynb_checkpoints\n",
      "feature.csv\n",
      "feature_rip.csv\n",
      "feature_ecg.csv\n",
      "feature_all.csv\n",
      "feature_rip_ecg.csv\n",
      "feature_ecg_norm.csv\n",
      "(6926, 51)\n"
     ]
    }
   ],
   "source": [
    "# Soujanya Chatterjee\n",
    "\t\n",
    "# 2:06 PM (9 minutes ago)\n",
    "\t\n",
    "# to me\n",
    "import pandas as pd, numpy as np, os, csv, glob, math, matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from datetime import datetime\n",
    "from scipy.stats import *\n",
    "import gzip\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def find_majority(k):\n",
    "    myMap = {}\n",
    "    maximum = ( '', 0 ) # (occurring element, occurrences)\n",
    "    for n in k:\n",
    "        if n in myMap: myMap[n] += 1\n",
    "        else: myMap[n] = 1\n",
    "\n",
    "        # Keep track of maximum on the go\n",
    "        if myMap[n] > maximum[1]: maximum = (n,myMap[n])\n",
    "\n",
    "    return maximum[0]\n",
    "\n",
    "# _dir = 'W:\\\\Students\\\\cstress_features\\\\data\\\\data\\\\SI02\\\\'\n",
    "\n",
    "def decodeLabel(label):\n",
    "    label = label[:2]  # Only the first 2 characters designate the label code\n",
    "\n",
    "    mapping = {'c1': 0, 'c2': 1, 'c3': 1, 'c4': 0, 'c5': 0, 'c6': 0, 'c7': 2}\n",
    "\n",
    "    return mapping[label]\n",
    "\n",
    "def readstressmarks(participantID, filename):\n",
    "    features = []\n",
    "    for file in os.listdir(filename):    \n",
    "        if file.endswith(\"marks.txt.gz\"):        \n",
    "            with gzip.open(os.path.join(filename, file), 'r') as file:\n",
    "                for line in file.readlines():\n",
    "                    line = line.decode('utf8').strip()\n",
    "                    parts = [x.strip() for x in line.split(',')]                    \n",
    "                    label = parts[0][:2]  \n",
    "                    if label not in ['c7','c6']:\n",
    "                        stressClass = decodeLabel(label)\n",
    "                        features.append([participantID, stressClass, int(parts[2]), int(parts[3])])\n",
    "    return np.array(features)\n",
    "\n",
    "_dirr = './data/'\n",
    "parti = np.array(os.listdir(_dirr) )\n",
    "header = ['participant','starttime','endtime','label'] + ['f_'+str(i) for i in range(47)]\n",
    "fea_cols = ['f_'+str(i) for i in range(47)]\n",
    "data = []\n",
    "for p in parti:\n",
    "    print(p)\n",
    "    if p in ['feature.csv','feature_ecg.csv','feature_rip.csv','feature_all.csv',\n",
    "             '.ipynb_checkpoints']:\n",
    "        continue\n",
    "    else:\n",
    "        if os.path.isdir(os.path.join(_dirr,p)):\n",
    "            _dir = (os.path.join(_dirr,p))\n",
    "            gt_marks = readstressmarks(p,_dir)\n",
    "            if len(gt_marks)==0:\n",
    "                continue\n",
    "            groundtruth = pd.DataFrame({'participant': gt_marks[:, 0], 'label': gt_marks[:, 1], 'starttime': gt_marks[:, 2],\n",
    "                                        'endtime': gt_marks[:, 3]}, columns=['participant','label','starttime','endtime'])\n",
    "            groundtruth = groundtruth.sort_values('starttime')\n",
    "   \n",
    "            check = False\n",
    "            for file in os.listdir(_dir):    \n",
    "                    if file.endswith('features_rip_ecg.p'):                    \n",
    "                        with open(_dir+'/'+file, 'rb') as f:  \n",
    "                            x = pickle.load(f)\n",
    "                            check  =True\n",
    "            if not check:\n",
    "                continue\n",
    "#             print(x.shape)\n",
    "#             dataset = pd.DataFrame({'starttime': x[:, 0], 'endtime': x[:, 1], 'f_1': x[:, 2]\n",
    "#                                    , 'f_2': x[:, 3], 'f_3': x[:, 4], 'f_4': x[:, 5]\n",
    "#                                    , 'f_5': x[:, 6], 'f_6': x[:, 7], 'f_7': x[:, 8]\n",
    "#                                    , 'f_8': x[:, 9], 'f_9': x[:, 10], 'f_10': x[:, 11]\n",
    "#                                    , 'f_11': x[:, 12]}, columns=['starttime','endtime','f_1','f_2','f_3','f_4','f_5','f_6','f_7','f_8',\n",
    "#                                                                  'f_9','f_10','f_11'])\n",
    "            \n",
    "            dataset = pd.DataFrame(x,columns=['starttime','endtime']+['f_'+str(i) for i in range(x.shape[1]-2)])\n",
    "            dataset = dataset.sort_values('starttime')\n",
    "\n",
    "            for gt in range(len(dataset)):\n",
    "                starttime = int(dataset['starttime'].iloc[gt])\n",
    "                endtime = int(dataset['endtime'].iloc[gt])\n",
    "                result = []\n",
    "                for line in range(len(groundtruth)):\n",
    "                    id, gtt, st, et = [groundtruth['participant'].iloc[line], groundtruth['label'].iloc[line], int(groundtruth['starttime'].iloc[line]),\n",
    "                                      int(groundtruth['endtime'].iloc[line])]\n",
    "                    if starttime < st:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (starttime > st) and (endtime < et):\n",
    "                            result.append(gtt)\n",
    "                        if result:\n",
    "                            fea = list(dataset[fea_cols].iloc[gt])\n",
    "                            inter_data = [p, st,et,find_majority(result)],(fea)\n",
    "                            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "                            data.append(flatten(inter_data))\n",
    "    #         print(data)\n",
    "df = pd.DataFrame(data)\n",
    "df.fillna(df.mean(),inplace=True)\n",
    "print(df.shape)\n",
    "df.to_csv(_dirr + '/' + 'feature_rip_ecg.csv', index=False, header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6926, 47) (6926,) 1347 [0 1] 17\n"
     ]
    }
   ],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "feature_file = './data/feature_rip_ecg.csv'\n",
    "feature = pd.read_csv(feature_file).values\n",
    "y = np.int64(feature[:,3])\n",
    "X = feature[:,4:]\n",
    "groups = feature[:,0]\n",
    "print(X.shape,y.shape,np.sum(y),np.unique(y),len(np.unique(groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8ecf867a9104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.hist(X[:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba',n_jobs=-1)[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator=estimator, param_grid=param_grid, scoring=scoring,  n_jobs=n_jobs, iid=iid,\n",
    "                refit=refit, cv=cv, verbose=verbose, pre_dispatch=pre_dispatch, error_score=error_score)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "        cv = self.cv\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "        if self.verbose > 0:\n",
    "#             if isinstance(parameter_iterable, Sized):\n",
    "            n_candidates = len(parameter_iterable)\n",
    "            print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "                  \" {2} fits\".format(len(cv), n_candidates,\n",
    "                                     n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 17 folds for each of 30 candidates, totalling 510 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done  13 out of  30 | elapsed:  3.5min remaining:  4.6min\n",
      "[Parallel(n_jobs=40)]: Done  30 out of  30 | elapsed:  4.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModifiedGridSearchCV(cv=[(array([   0,    1,    2, ..., 6923, 6924, 6925]),\n",
       "                          array([1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718,\n",
       "       1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729,\n",
       "       1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740,\n",
       "       1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751,\n",
       "       1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762,\n",
       "       1763, 176...\n",
       "                                                      {0: 0.4, 1: 0.6}],\n",
       "                                 'rf__gamma': array([1.00000000e-03, 4.64158883e-03, 2.15443469e-02, 1.00000000e-01,\n",
       "       4.64158883e-01, 2.15443469e+00, 1.00000000e+01, 4.64158883e+01,\n",
       "       2.15443469e+02, 1.00000000e+03]),\n",
       "                                 'rf__kernel': ['rbf'],\n",
       "                                 'rf__probability': [True],\n",
       "                                 'rf__verbose': [False]},\n",
       "                     pre_dispatch='2*n_jobs', refit=True,\n",
       "                     scoring=<function f1Bias_scorer_CV at 0x7fb0edb4b510>,\n",
       "                     verbose=1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "# X1 = StandardScaler().fit_transform(X)\n",
    "X1 = X\n",
    "delta = 0.2\n",
    "parameters1 = {'rf__kernel': ['rbf'],\n",
    "              'rf__C': [10],\n",
    "              'rf__gamma': np.logspace(-3,3,10),\n",
    "              'rf__class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, .50, delta)],\n",
    "              'rf__probability':[True],\n",
    "              'rf__verbose':[False],\n",
    "              'rf__cache_size':[2000]}\n",
    "parameters = {\n",
    "    'rf__min_samples_leaf': [4],\n",
    "    'rf__max_features': [1],\n",
    "    'rf__n_estimators': [100,200],\n",
    "    'rf__n_jobs': [-1],\n",
    "    'rf__criterion':['gini','entropy'],\n",
    "    'rf__class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.01, .6, delta)],\n",
    "    'rf__random_state': [42]\n",
    "       }\n",
    "parameters = {\n",
    "    'rf__C':np.logspace(-3,3,100),\n",
    "#     'rf__min_samples_leaf': [4],\n",
    "#     'rf__max_features': [1],\n",
    "#     'rf__n_estimators': [100,200],\n",
    "    'rf__n_jobs': [-1],\n",
    "#     'rf__criterion':['gini','entropy'],\n",
    "    'rf__class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.01, .6, delta)],\n",
    "#     'rf__random_state': [42]\n",
    "       }\n",
    "svc = Pipeline([('sts',preprocessing.StandardScaler()),('rf',RandomForestClassifier())])\n",
    "svc = Pipeline([('sts',preprocessing.StandardScaler()),('rf',LogisticRegression())])\n",
    "svc = Pipeline([('sts',preprocessing.StandardScaler()),('pca',PCA(n_components=10)),('rf',SVC())])\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X1,y,groups=groups)),\n",
    "                                   n_jobs=40, scoring=f1Bias_scorer_CV, verbose=1, iid=False)\n",
    "grid_search.fit(X1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8286626463284853 0.32807360211968634\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      5579\n",
      "           1       0.79      0.87      0.83      1347\n",
      "\n",
      "    accuracy                           0.93      6926\n",
      "   macro avg       0.88      0.91      0.89      6926\n",
      "weighted avg       0.93      0.93      0.93      6926\n",
      "\n",
      "[[5275  304]\n",
      " [ 179 1168]]\n",
      "0.84375\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "clf\n",
    "\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "# clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X1, y, gkf.split(X1,y,groups=groups))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(CV_probs,50)\n",
    "plt.show()\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6926, 47)\n"
     ]
    }
   ],
   "source": [
    "print(X1.shape)\n",
    "clf.fit(X1,y)\n",
    "import pickle\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/apply_cstress_to_rice/models/ecg_rip_model_final.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "paramGrid = {\n",
    "             'pca__n_components':[4,5,6],\n",
    "             'rf__kernel': ['rbf'],\n",
    "             'rf__C': np.logspace(.1,4,3),\n",
    "             'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-4, 0, .5)],\n",
    "             'rf__class_weight': [{0: w, 1: 1 - w} for w in [.4,.3,.2]],\n",
    "             'rf__probability':[True]\n",
    "}\n",
    "\n",
    "paramGrid = {\n",
    "             'kernel': ['rbf'],\n",
    "             'C': np.logspace(.1,4,3),\n",
    "             'gamma': [np.power(2,np.float(x)) for x in np.arange(-4, 0, .5)],\n",
    "             'class_weight': [{0: w, 1: 1 - w} for w in [.4,.3,.2]],\n",
    "             'probability':[True]\n",
    "}\n",
    "# clf = Pipeline([('pca',PCA()),('rf', SVC())])\n",
    "clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='f1_weighted',verbose=5)\n",
    "grid_search.fit(X,y)\n",
    "\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups))\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(clf)\n",
    "clf.fit(X,y)\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/cc3/ecg_rip_model_feature_standardization_minnesota.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_porter import Porter\n",
    "porter = Porter(clf, language='java')\n",
    "output = porter.export(export_data=True)\n",
    "# print(output)\n",
    "text_file = open(\"SVM.java\", \"w\")\n",
    "text_file.write(output)\n",
    "text_file.close()\n",
    "print(clf.probA_,clf.probB_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3.3",
   "language": "python",
   "name": "cc33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
