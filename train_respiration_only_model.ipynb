{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "path_to_zip_file = 'data.zip'\n",
    "directory_to_extract_to = './data/'\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions, Quality Annotation, Peak Valley Detection, Feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tomkin import detect_rpeak\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from outlier_calculation import Quality,compute_outlier_ecg\n",
    "from joblib import Parallel,delayed\n",
    "from data_quality import ECGQualityCalculation\n",
    "from joblib import delayed,Parallel\n",
    "from copy import deepcopy\n",
    "from ecg import ecg_feature_computation\n",
    "from scipy import interpolate\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gzip\n",
    "# from peak_valley import compute_peak_valley\n",
    "\n",
    "def get_interpolated(aclx,acly,aclz):\n",
    "    time_array = aclx[:,1].reshape(-1,1)\n",
    "    aclxyz = np.concatenate([time_array,time_array,time_array,time_array],axis=1)\n",
    "    f = interpolate.interp1d(aclx[:,1],aclx[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,1] = f(aclxyz[:,0])\n",
    "    f = interpolate.interp1d(acly[:,1],acly[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,2] = f(aclxyz[:,0])\n",
    "    f = interpolate.interp1d(aclz[:,1],aclz[:,0],fill_value='extrapolate')\n",
    "    aclxyz[:,3] = f(aclxyz[:,0])\n",
    "    return aclxyz\n",
    "\n",
    "def get_quality(data):\n",
    "    outlier_threshold_high = 4000\n",
    "    outlier_threshold_low = 20\n",
    "    slope_threshold = 300\n",
    "    eck_threshold_band_loose = 175\n",
    "    eck_threshold_band_off = 20\n",
    "    minimum_expected_samples = 3*(0.33)*21.33\n",
    "    data_quality_band_loose = 0\n",
    "    data_quality_not_worn = 0\n",
    "    data_quality_band_off = 0\n",
    "    data_quality_missing = 0 \n",
    "    data_quality_good = 1\n",
    "    acceptable_outlier_percent = 34\n",
    "    \n",
    "    if (len(data)== 0):\n",
    "        return data_quality_band_off\n",
    "    range_data = max(data)-min(data)\n",
    "    if range_data<=eck_threshold_band_off:\n",
    "        return data_quality_not_worn\n",
    "    if (len(data)<=minimum_expected_samples) :\n",
    "        return data_quality_missing\n",
    "    if range_data<=eck_threshold_band_loose:\n",
    "        return data_quality_band_loose\n",
    "    \n",
    "    outlier_counts = 0 \n",
    "    for i in range(0,len(data)):\n",
    "        im,ip  = i,i\n",
    "        if i==0:\n",
    "            im = len(data)-1\n",
    "        else:\n",
    "            im = i-1\n",
    "        if i == len(data)-1:\n",
    "            ip = 0\n",
    "        else:\n",
    "            ip = ip+1\n",
    "        stuck = ((data[i]==data[im]) and (data[i]==data[ip]))\n",
    "        flip = ((abs(data[i]-data[im])>((int(outlier_threshold_high)))) or (abs(data[i]-data[ip])>((int(outlier_threshold_high)))))\n",
    "        disc = ((abs(data[i]-data[im])>((int(slope_threshold)))) and (abs(data[i]-data[ip])>((int(slope_threshold)))))\n",
    "        if disc:\n",
    "            outlier_counts += 1\n",
    "        elif stuck:\n",
    "            outlier_counts +=1\n",
    "        elif flip:\n",
    "            outlier_counts +=1\n",
    "        elif data[i] >= outlier_threshold_high:\n",
    "            outlier_counts +=1\n",
    "        elif data[i]<= outlier_threshold_low:\n",
    "            outlier_counts +=1\n",
    "    if (100*outlier_counts>acceptable_outlier_percent*len(data)):\n",
    "        return data_quality_band_loose\n",
    "    return data_quality_good\n",
    "\n",
    "def get_clean_ecg(ecg_data):\n",
    "    final_data = np.zeros((0,3))\n",
    "    if len(ecg_data)==0:\n",
    "        return final_data\n",
    "    test_object = ECGQualityCalculation()\n",
    "    start_ts = ecg_data[0,0]\n",
    "    final_data = np.zeros((0,3))\n",
    "    while start_ts<ecg_data[-1,0]:\n",
    "        index = np.where((ecg_data[:,0]>=start_ts)&(ecg_data[:,0]<start_ts+3000))[0]\n",
    "        temp_data = ecg_data[index,2]\n",
    "        if test_object.current_quality(temp_data)==1:\n",
    "            final_data = np.concatenate((final_data,ecg_data[index,:]))\n",
    "        start_ts = start_ts + 3000\n",
    "    return final_data\n",
    "\n",
    "def get_clean_rip(rip_data):\n",
    "    if len(rip_data)==0:\n",
    "        return final_data\n",
    "    i = 0\n",
    "    final_data = np.zeros((0,2))\n",
    "    while i<len(rip_data):\n",
    "        j = i\n",
    "        index = []\n",
    "        while j<len(rip_data) and rip_data[j,0] - rip_data[i,0] <= 3000:\n",
    "            index.append(j)\n",
    "            j+=1\n",
    "        i=j+1\n",
    "        temp_data = rip_data[np.array(index),1]\n",
    "        if get_quality(temp_data)==1:\n",
    "            final_data = np.concatenate((final_data,rip_data[np.array(index),:]))\n",
    "    return final_data\n",
    "\n",
    "def get_hr(ecg_data):\n",
    "#     try:\n",
    "    rpeaks = detect_rpeak(ecg_data[:,2],64)\n",
    "    rpeak_ts = ecg_data[rpeaks,0]\n",
    "    ecg_rr = np.zeros((len(rpeaks)-1,2))\n",
    "    ecg_rr_ts = np.array(rpeak_ts)[1:]\n",
    "    ecg_rr_sample = np.array(np.diff(rpeak_ts))\n",
    "    index = np.where((ecg_rr_sample>=300)&(ecg_rr_sample<=2000))[0]\n",
    "    ecg_rr_ts = ecg_rr_ts[index]\n",
    "    ecg_rr_sam = ecg_rr_sample[index]\n",
    "    rr = remove_ectopic_beats(ecg_rr_sam)\n",
    "    ecg_rr_sam = ecg_rr_sam[~np.isnan(rr)]\n",
    "    ecg_rr_ts = ecg_rr_ts[~np.isnan(rr)]\n",
    "    outlier = compute_outlier_ecg(ecg_rr_ts/1000,ecg_rr_sam/1000)\n",
    "    ind1 = []\n",
    "    for ind,tup in enumerate(outlier):\n",
    "        if tup[1]==Quality.ACCEPTABLE:\n",
    "            ind1.append(ind)\n",
    "    ind1 = np.array(ind1)\n",
    "    if len(ind1)<100:\n",
    "        return [],[]\n",
    "    ecg_rr_ts = ecg_rr_ts[ind1]\n",
    "    ecg_rr_sam = ecg_rr_sam[ind1]\n",
    "    return ecg_rr_ts,ecg_rr_sam\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "def get_windows(data,window_size=10,offset=10,fs=1):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>t-window_size*1000/2)&(data[:,0]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<50:\n",
    "            continue\n",
    "        window_col.append(data[index,:])\n",
    "    return window_col\n",
    "\n",
    "def get_rip_windows(data,window_size=60,offset=10,fs=.2):\n",
    "    ts_array = np.arange(data[0,0],data[-1,0],offset*1000)\n",
    "    window_col = []\n",
    "    for t in ts_array:\n",
    "        index = np.where((data[:,0]>=t-window_size*1000/2)&(data[:,1]<=t+window_size*1000/2))[0]\n",
    "        if len(index)<10:\n",
    "            continue\n",
    "        window_col.append(data[index,:])\n",
    "    return window_col\n",
    "\n",
    "\n",
    "from peak_valley import compute_peak_valley,rip_cycle_feature_computation\n",
    "from scipy import stats\n",
    "\n",
    "def get_std_chest(window,start=1,end=4):\n",
    "    return np.array([np.mean(window[:,0]),np.sqrt(np.sum(np.power(np.std(window[:,start:end],axis=0),2)))])\n",
    "def filter_ecg_windows(ecg_windows,acl_std):\n",
    "    final_ecg_windows = []\n",
    "    for window in ecg_windows:\n",
    "        index = np.where((acl_std[:,0]>window[0,0])&(acl_std[:,0]<window[-1,0]))[0]\n",
    "        if len(index)==0:\n",
    "            continue\n",
    "        window_temp = acl_std[index,1].reshape(-1)\n",
    "        if len(window_temp[window_temp>.21])/len(window_temp) > .5:\n",
    "            continue\n",
    "        final_ecg_windows.append(window)\n",
    "    return final_ecg_windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "participants = [path + f +'/' for f in os.listdir(path) if f[0]=='S']\n",
    "count=0\n",
    "for f in participants:\n",
    "    try:\n",
    "        rip = pd.read_csv(f  +'rip.txt.gz', compression='gzip',\n",
    "                          sep=' ',header=None).values\n",
    "        a = deepcopy(rip[:,0])\n",
    "        rip[:,0] = rip[:,1]\n",
    "        rip[:,1] = a\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    count+=1\n",
    "    rip_data = get_clean_rip(rip)\n",
    "    pickle.dump(rip_data,open(f+'rip.p','wb'))\n",
    "    rip = pickle.load(open(f+'rip.p','rb'))\n",
    "    \n",
    "    peaks,valleys = compute_peak_valley(rip)\n",
    "    print(peaks.shape,valleys.shape,peaks[:2])\n",
    "    pickle.dump([peaks,valleys],open(f+'pv.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209, 38)\n",
      "(250, 38)\n",
      "(229, 38)\n",
      "(263, 38)\n",
      "(216, 38)\n",
      "(256, 38)\n",
      "(271, 38)\n",
      "(201, 38)\n",
      "(269, 38)\n",
      "(255, 38)\n",
      "(259, 38)\n",
      "(293, 38)\n",
      "(264, 38)\n",
      "(233, 38)\n",
      "(274, 38)\n",
      "(255, 38)\n",
      "(306, 38)\n",
      "(240, 38)\n",
      "(256, 38)\n",
      "(255, 38)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "path = './data/'\n",
    "participants = [path + f +'/' for f in os.listdir(path) if f[0]=='S']\n",
    "count=0\n",
    "for f in participants[::-1]: \n",
    "    if 'pv.p' not in os.listdir(f) or 'stress_marks.txt.gz' not in os.listdir(f):\n",
    "        continue\n",
    "    st = 0\n",
    "    et = 0 \n",
    "    with gzip.open(f+'stress_marks.txt.gz', 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            line = line.decode('utf8').strip()\n",
    "            parts = [x.strip() for x in line.split(',')]\n",
    "            label = parts[0]\n",
    "            if label[:2] in ['c1']:\n",
    "                st = np.int64(parts[2])\n",
    "                et = np.int64(parts[3])\n",
    "\n",
    "    peaks,valleys = pickle.load(open(f+'pv.p','rb'))\n",
    "    rip_features = rip_cycle_feature_computation(peaks,valleys)\n",
    "    rip_features = rip_features[:,np.array([0,1,2,3,4,5,6,7,8,-2,-1])]\n",
    "    for c in range(2,rip_features.shape[1]):\n",
    "        rip_features[:,c] = StandardScaler().fit_transform(rip_features[:,c].reshape(-1,1)).reshape(-1)\n",
    "        rip_features[rip_features[:,c]>5,c] = 5\n",
    "        rip_features[rip_features[:,c]<-5,c] = -5\n",
    "        \n",
    "#         rip_features[:,c] = stats.mstats.winsorize(rip_features[:,c],limits=.01)\n",
    "    rip_windows = get_rip_windows(rip_features,window_size=60,offset=30,fs=1)\n",
    "    rip_features_minute = np.array([np.array([window[0,0],window[-1,1]]+\n",
    "                                              list(np.mean(window[:,2:],axis=0))+\n",
    "                                              list(np.std(window[:,2:],axis=0))+\n",
    "                                              list(np.percentile(window[:,2:],80,axis=0))+\n",
    "                                              list(np.percentile(window[:,2:],20,axis=0)))  for window \n",
    "                                              in rip_windows])\n",
    "    rip_features_minute[:,2:] =StandardScaler().fit_transform(rip_features_minute[:,2:])\n",
    "    print(rip_features_minute.shape)\n",
    "    pickle.dump(rip_features_minute,open(f+'features_rip2.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.224e+03, 1.110e+02, 1.700e+01, 1.200e+01, 4.000e+00, 3.000e+00,\n",
       "        1.000e+00, 4.000e+00, 0.000e+00, 4.000e+00]),\n",
       " array([-0.29526691,  0.23425978,  0.76378647,  1.29331316,  1.82283985,\n",
       "         2.35236654,  2.88189324,  3.41141993,  3.94094662,  4.47047331,\n",
       "         5.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.hist([:,10])\n",
    "plt.hist(rip_features[:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotate each feature row with stress/not stress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "SI01\n",
      "SI02\n",
      "SI03\n",
      "SI04\n",
      "SI05\n",
      "SI06\n",
      "SI07\n",
      "SI08\n",
      "SI09\n",
      "SI10\n",
      "SI11\n",
      "SI12\n",
      "SI13\n",
      "SI14\n",
      "SI15\n",
      "SI16\n",
      "SI17\n",
      "SI18\n",
      "SI19\n",
      "SI20\n",
      "SI21\n",
      "SI22\n",
      "SI23\n",
      "SI24\n",
      ".ipynb_checkpoints\n",
      "feature.csv\n",
      "feature_rip.csv\n",
      "feature_ecg.csv\n",
      "feature_all.csv\n",
      "feature_rip_ecg.csv\n",
      "feature_ecg_norm.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SI02</td>\n",
       "      <td>1265820695499</td>\n",
       "      <td>1265820906661</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.440929</td>\n",
       "      <td>1.119819</td>\n",
       "      <td>-0.654823</td>\n",
       "      <td>-1.259305</td>\n",
       "      <td>0.475557</td>\n",
       "      <td>0.117072</td>\n",
       "      <td>...</td>\n",
       "      <td>0.856622</td>\n",
       "      <td>-1.633977</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>-1.719718</td>\n",
       "      <td>-1.484075</td>\n",
       "      <td>-0.445609</td>\n",
       "      <td>0.071472</td>\n",
       "      <td>-5.185084</td>\n",
       "      <td>-1.985656</td>\n",
       "      <td>-0.739230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SI02</td>\n",
       "      <td>1265820695499</td>\n",
       "      <td>1265820906661</td>\n",
       "      <td>0</td>\n",
       "      <td>1.552159</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>1.473430</td>\n",
       "      <td>1.291964</td>\n",
       "      <td>0.579846</td>\n",
       "      <td>5.443460</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119218</td>\n",
       "      <td>-1.429268</td>\n",
       "      <td>0.124058</td>\n",
       "      <td>-1.695304</td>\n",
       "      <td>-1.140569</td>\n",
       "      <td>-0.514213</td>\n",
       "      <td>0.401107</td>\n",
       "      <td>-0.976036</td>\n",
       "      <td>-2.008500</td>\n",
       "      <td>-0.664996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SI02</td>\n",
       "      <td>1265821586604</td>\n",
       "      <td>1265822107628</td>\n",
       "      <td>0</td>\n",
       "      <td>1.005073</td>\n",
       "      <td>-0.443625</td>\n",
       "      <td>0.670907</td>\n",
       "      <td>1.041743</td>\n",
       "      <td>1.267563</td>\n",
       "      <td>-7.097754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.951957</td>\n",
       "      <td>-0.481461</td>\n",
       "      <td>-0.564165</td>\n",
       "      <td>-0.872865</td>\n",
       "      <td>0.296551</td>\n",
       "      <td>-0.822927</td>\n",
       "      <td>-5.931339</td>\n",
       "      <td>-0.046005</td>\n",
       "      <td>-0.708625</td>\n",
       "      <td>-2.299083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SI02</td>\n",
       "      <td>1265821586604</td>\n",
       "      <td>1265822107628</td>\n",
       "      <td>0</td>\n",
       "      <td>1.012331</td>\n",
       "      <td>0.199391</td>\n",
       "      <td>1.085841</td>\n",
       "      <td>0.072687</td>\n",
       "      <td>0.446864</td>\n",
       "      <td>-0.752681</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777197</td>\n",
       "      <td>-0.744658</td>\n",
       "      <td>-0.346683</td>\n",
       "      <td>-1.039565</td>\n",
       "      <td>0.134144</td>\n",
       "      <td>-0.704391</td>\n",
       "      <td>-2.777266</td>\n",
       "      <td>-0.215215</td>\n",
       "      <td>-0.641257</td>\n",
       "      <td>-1.701984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SI02</td>\n",
       "      <td>1265821586604</td>\n",
       "      <td>1265822107628</td>\n",
       "      <td>0</td>\n",
       "      <td>1.734140</td>\n",
       "      <td>-0.452336</td>\n",
       "      <td>1.356260</td>\n",
       "      <td>1.181156</td>\n",
       "      <td>-1.102788</td>\n",
       "      <td>-0.501237</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530503</td>\n",
       "      <td>1.984788</td>\n",
       "      <td>0.216323</td>\n",
       "      <td>1.433857</td>\n",
       "      <td>1.637395</td>\n",
       "      <td>-0.821624</td>\n",
       "      <td>-0.395511</td>\n",
       "      <td>-0.202327</td>\n",
       "      <td>-0.297429</td>\n",
       "      <td>0.378396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0              1              2  3         4         5         6   \\\n",
       "0  SI02  1265820695499  1265820906661  0 -1.440929  1.119819 -0.654823   \n",
       "1  SI02  1265820695499  1265820906661  0  1.552159  0.004048  1.473430   \n",
       "2  SI02  1265821586604  1265822107628  0  1.005073 -0.443625  0.670907   \n",
       "3  SI02  1265821586604  1265822107628  0  1.012331  0.199391  1.085841   \n",
       "4  SI02  1265821586604  1265822107628  0  1.734140 -0.452336  1.356260   \n",
       "\n",
       "         7         8         9   ...        30        31        32        33  \\\n",
       "0 -1.259305  0.475557  0.117072  ...  0.856622 -1.633977  0.007314 -1.719718   \n",
       "1  1.291964  0.579846  5.443460  ...  1.119218 -1.429268  0.124058 -1.695304   \n",
       "2  1.041743  1.267563 -7.097754  ...  0.951957 -0.481461 -0.564165 -0.872865   \n",
       "3  0.072687  0.446864 -0.752681  ...  0.777197 -0.744658 -0.346683 -1.039565   \n",
       "4  1.181156 -1.102788 -0.501237  ... -0.530503  1.984788  0.216323  1.433857   \n",
       "\n",
       "         34        35        36        37        38        39  \n",
       "0 -1.484075 -0.445609  0.071472 -5.185084 -1.985656 -0.739230  \n",
       "1 -1.140569 -0.514213  0.401107 -0.976036 -2.008500 -0.664996  \n",
       "2  0.296551 -0.822927 -5.931339 -0.046005 -0.708625 -2.299083  \n",
       "3  0.134144 -0.704391 -2.777266 -0.215215 -0.641257 -1.701984  \n",
       "4  1.637395 -0.821624 -0.395511 -0.202327 -0.297429  0.378396  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Soujanya Chatterjee\n",
    "\t\n",
    "# 2:06 PM (9 minutes ago)\n",
    "\t\n",
    "# to me\n",
    "import pandas as pd, numpy as np, os, csv, glob, math, matplotlib.pyplot as plt\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from datetime import datetime\n",
    "from scipy.stats import *\n",
    "import gzip\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def find_majority(k):\n",
    "    myMap = {}\n",
    "    maximum = ( '', 0 ) # (occurring element, occurrences)\n",
    "    for n in k:\n",
    "        if n in myMap: myMap[n] += 1\n",
    "        else: myMap[n] = 1\n",
    "\n",
    "        # Keep track of maximum on the go\n",
    "        if myMap[n] > maximum[1]: maximum = (n,myMap[n])\n",
    "\n",
    "    return maximum[0]\n",
    "\n",
    "# _dir = 'W:\\\\Students\\\\cstress_features\\\\data\\\\data\\\\SI02\\\\'\n",
    "\n",
    "def decodeLabel(label):\n",
    "    label = label[:2]  # Only the first 2 characters designate the label code\n",
    "\n",
    "    mapping = {'c1': 0, 'c2': 1, 'c3': 1, 'c4': 0, 'c5': 0, 'c6': 0, 'c7': 2}\n",
    "\n",
    "    return mapping[label]\n",
    "\n",
    "def readstressmarks(participantID, filename):\n",
    "    features = []\n",
    "    for file in os.listdir(filename):    \n",
    "        if file.endswith(\"marks.txt.gz\"):        \n",
    "            with gzip.open(os.path.join(filename, file), 'r') as file:\n",
    "                for line in file.readlines():\n",
    "                    line = line.decode('utf8').strip()\n",
    "                    parts = [x.strip() for x in line.split(',')]                    \n",
    "                    label = parts[0][:2]  \n",
    "                    if label not in ['c7','c6']:\n",
    "                        stressClass = decodeLabel(label)\n",
    "                        features.append([participantID, stressClass, int(parts[2]), int(parts[3])])\n",
    "    return np.array(features)\n",
    "_dirr = './data/'\n",
    "parti = np.array(os.listdir(_dirr) )\n",
    "header = ['participant','starttime','endtime','label'] + ['f_'+str(i) for i in range(36)]\n",
    "fea_cols = ['f_'+str(i) for i in range(36)]\n",
    "data = []\n",
    "for p in parti:\n",
    "    print(p)\n",
    "    if p in ['feature.csv','feature_ecg.csv','feature_rip.csv',\n",
    "             'SI09','SI23','SI24','.ipynb_checkpoints']:\n",
    "        continue\n",
    "    else:\n",
    "        if os.path.isdir(os.path.join(_dirr,p)):\n",
    "           \n",
    "            _dir = (os.path.join(_dirr,p))\n",
    "            gt_marks = readstressmarks(p,_dir)\n",
    "            groundtruth = pd.DataFrame({'participant': gt_marks[:, 0], 'label': gt_marks[:, 1], 'starttime': gt_marks[:, 2],\n",
    "                                        'endtime': gt_marks[:, 3]}, columns=['participant','label','starttime','endtime'])\n",
    "            groundtruth = groundtruth.sort_values('starttime')\n",
    "   \n",
    "            check = False\n",
    "            for file in os.listdir(_dir):    \n",
    "                    if file.endswith(\"features_rip2.p\"):                    \n",
    "                        with open(_dir+'/'+file, 'rb') as f:  \n",
    "                            x = pickle.load(f)\n",
    "                            check  =True\n",
    "            if not check:\n",
    "                continue\n",
    "#             print(x.shape)\n",
    "#             dataset = pd.DataFrame({'starttime': x[:, 0], 'endtime': x[:, 1], 'f_1': x[:, 2]\n",
    "#                                    , 'f_2': x[:, 3], 'f_3': x[:, 4], 'f_4': x[:, 5]\n",
    "#                                    , 'f_5': x[:, 6], 'f_6': x[:, 7], 'f_7': x[:, 8]\n",
    "#                                    , 'f_8': x[:, 9], 'f_9': x[:, 10], 'f_10': x[:, 11]\n",
    "#                                    , 'f_11': x[:, 12]}, columns=['starttime','endtime','f_1','f_2','f_3','f_4','f_5','f_6','f_7','f_8',\n",
    "#                                                                  'f_9','f_10','f_11'])\n",
    "\n",
    "            dataset = pd.DataFrame(x,columns=['starttime','endtime']+['f_'+str(i) for i in range(x.shape[1]-2)])\n",
    "            dataset = dataset.sort_values('starttime')\n",
    "\n",
    "            for gt in range(len(dataset)):\n",
    "                starttime = int(dataset['starttime'].iloc[gt])\n",
    "                endtime = int(dataset['endtime'].iloc[gt])\n",
    "                result = []\n",
    "                for line in range(len(groundtruth)):\n",
    "                    id, gtt, st, et = [groundtruth['participant'].iloc[line], groundtruth['label'].iloc[line], int(groundtruth['starttime'].iloc[line]),\n",
    "                                      int(groundtruth['endtime'].iloc[line])]\n",
    "                    if starttime < st:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (starttime > st) and (endtime < et):\n",
    "                            result.append(gtt)\n",
    "                        if result:\n",
    "                            fea = list(dataset[fea_cols].iloc[gt])\n",
    "                            inter_data = [p, st,et,find_majority(result)],(fea)\n",
    "                            flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "                            data.append(flatten(inter_data))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.to_csv(_dirr + '/' + 'feature_rip.csv', index=False, header=header)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the saved feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2789, 36) (2789,) 535 [0 1]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "feature_file = './data/feature_rip.csv'\n",
    "feature = pd.read_csv(feature_file).values\n",
    "y = np.int64(feature[:,3])\n",
    "X = feature[:,4:]\n",
    "print(X.shape,y.shape,np.sum(y),np.unique(y))\n",
    "groups = feature[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 43., 828., 922., 547., 253., 136.,  42.,  14.,   3.,   1.]),\n",
       " array([-2.3238547666285494, -1.523975351586486, -0.7240959365444226,\n",
       "        0.07578347849764056, 0.8756628935397042, 1.6755423085817678,\n",
       "        2.4754217236238305, 3.275301138665894, 4.075180553707957,\n",
       "        4.875059968750021, 5.674939383792085], dtype=object),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X[X<=-5] = -5\n",
    "# X[X>=5] = 5\n",
    "plt.hist(X[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave one Subject Out Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from joblib import Parallel,delayed\n",
    "\n",
    "delta = 0.1\n",
    "\n",
    "paramGrid = {\n",
    "             'rf__kernel': ['rbf'],\n",
    "             'rf__C': np.logspace(.1,4,5),\n",
    "             'rf__gamma': [np.power(2,np.float(x)) for x in np.arange(-4, 4,1)],\n",
    "             'rf__class_weight': [{0: w, 1: 1 - w} for w in [.4,.3,.2,.2]],\n",
    "             'rf__probability':[False]\n",
    "}\n",
    "clf = Pipeline([('pca',StandardScaler()),('rf', SVC())])\n",
    "# clf = SVC()\n",
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "grid_search = GridSearchCV(clf, paramGrid, n_jobs=-1,cv=list(gkf.split(X,y,groups=groups)),\n",
    "                           scoring='accuracy',verbose=5)\n",
    "grid_search.fit(X,y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "clf = grid_search.best_estimator_\n",
    "clf.set_params(rf__probability=True) \n",
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),n_jobs=-1)\n",
    "print(confusion_matrix(y,y_pred),classification_report(y,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(clf)\n",
    "clf.fit(X,y)\n",
    "# pickle.dump(clf,open('/home/jupyter/mullah/cc3/rip_model_feature_standardization.p','wb'))\n",
    "\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/apply_cstress_to_rice/models/rip_model1.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(clf,X,y,cv=gkf.split(X,y,groups=groups),method='predict_proba',n_jobs=-1)[:,1]\n",
    "# print(confusion_matrix(y,y_pred),classification_report(y,y_pred))\n",
    "plt.hist(y_pred,100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cerebralcortex/kessel_jupyter_virtualenv/cc33/lib64/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "### from sklearn.decomposition import PCA\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "# import parfit.parfit as pf\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "# from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score,accuracy_score,classification_report\n",
    "import itertools\n",
    "from sklearn.model_selection import ParameterGrid, cross_val_predict, GroupKFold,GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from imblearn.pipeline import Pipeline\n",
    "import warnings\n",
    "from sklearn.model_selection import check_cv\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, ParameterSampler, ParameterGrid\n",
    "from sklearn.utils.validation import _num_samples, indexable\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "def Twobias_scorer_CV(probs, y, ret_bias=False):\n",
    "    db = np.transpose(np.vstack([np.array(probs).reshape(-1), np.array(y).reshape(-1)]))\n",
    "    db = db[np.argsort(db[:, 0]), :]\n",
    "\n",
    "    pos = np.sum(y == 1)\n",
    "    n = len(y)\n",
    "    neg = n - pos\n",
    "    tp, tn = pos, 0\n",
    "    lost = 0\n",
    "\n",
    "    optbias = []\n",
    "    minloss = 1\n",
    "\n",
    "    for i in range(n):\n",
    "        #\t\tp = db[i,1]\n",
    "        if db[i, 1] == 1:  # positive\n",
    "            tp -= 1.0\n",
    "        else:\n",
    "            tn += 1.0\n",
    "\n",
    "        # v1 = tp/pos\n",
    "        #\t\tv2 = tn/neg\n",
    "        if tp / pos >= 0.95 and tn / neg >= 0.95:\n",
    "            optbias = [db[i, 0], db[i, 0]]\n",
    "            continue\n",
    "\n",
    "        running_pos = pos\n",
    "        running_neg = neg\n",
    "        running_tp = tp\n",
    "        running_tn = tn\n",
    "\n",
    "        for j in range(i + 1, n):\n",
    "            #\t\t\tp1 = db[j,1]\n",
    "            if db[j, 1] == 1:  # positive\n",
    "                running_tp -= 1.0\n",
    "                running_pos -= 1\n",
    "            else:\n",
    "                running_neg -= 1\n",
    "\n",
    "            lost = (j - i) * 1.0 / n\n",
    "            if running_pos == 0 or running_neg == 0:\n",
    "                break\n",
    "\n",
    "            # v1 = running_tp/running_pos\n",
    "            #\t\t\tv2 = running_tn/running_neg\n",
    "\n",
    "            if running_tp / running_pos >= 0.95 and running_tn / running_neg >= 0.95 and lost < minloss:\n",
    "                minloss = lost\n",
    "                optbias = [db[i, 0], db[j, 0]]\n",
    "\n",
    "    if ret_bias:\n",
    "        return -minloss, optbias\n",
    "    else:\n",
    "        return -minloss\n",
    "def cv_fit_and_score(estimator, X, y, scorer, parameters, cv):\n",
    "    \"\"\"Fit estimator and compute scores for a given dataset split.\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object implementing 'fit'\n",
    "        The object to use to fit the data.\n",
    "    X : array-like of shape at least 2D\n",
    "        The data to fit.\n",
    "    y : array-like, optional, default: None\n",
    "        The target variable to try to predict in the case of\n",
    "        supervised learning.\n",
    "    scorer : callable\n",
    "        A scorer callable object / function with signature\n",
    "        ``scorer(estimator, X, y)``.\n",
    "    parameters : dict or None\n",
    "        Parameters to be set on the estimator.\n",
    "    cv:\tCross-validation fold indeces\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "        CV score on whole set.\n",
    "    parameters : dict or None, optional\n",
    "        The parameters that have been evaluated.\n",
    "    \"\"\"\n",
    "    estimator.set_params(**parameters)\n",
    "    cv_probs_ = cross_val_probs(estimator, X, y, cv)\n",
    "    score = scorer(cv_probs_, y)\n",
    "\n",
    "    return [score, parameters]  # scoring_time\n",
    "    \n",
    "def cross_val_probs(estimator, X, y, cv):\n",
    "    probs = np.zeros(len(y))\n",
    "    probs = cross_val_predict(estimator, X, y, cv=cv,method='predict_proba',n_jobs=-1)[:,1]\n",
    "#     for train, test in cv:\n",
    "#         temp = estimator.fit(X[train], y[train]).predict_proba(X[test])\n",
    "#         probs[test] = temp[:, 1]\n",
    "\n",
    "    return probs\n",
    "\n",
    "def f1Bias_scorer_CV(probs, y, ret_bias=False):\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y, probs)\n",
    "\n",
    "    f1 = 0.0\n",
    "    for i in range(0, len(thresholds)):\n",
    "        if not (precision[i] == 0 and recall[i] == 0):\n",
    "            f = 2 * (precision[i] * recall[i]) / (precision[i] + recall[i])\n",
    "            if f > f1:\n",
    "                f1 = f\n",
    "                bias = thresholds[i]\n",
    "\n",
    "    if ret_bias:\n",
    "        return f1, bias\n",
    "    else:\n",
    "        return f1\n",
    "    \n",
    "class ModifiedGridSearchCV(GridSearchCV):\n",
    "    def __init__(self, estimator, param_grid, scoring=None, fit_params=None,\n",
    "                 n_jobs=1, iid=True, refit=True, cv=None, verbose=0,\n",
    "                 pre_dispatch='2*n_jobs', error_score='raise'):\n",
    "\n",
    "        super(ModifiedGridSearchCV, self).__init__(\n",
    "                estimator=estimator, param_grid=param_grid, scoring=scoring,  n_jobs=n_jobs, iid=iid,\n",
    "                refit=refit, cv=cv, verbose=verbose, pre_dispatch=pre_dispatch, error_score=error_score)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Actual fitting,  performing the search over parameters.\"\"\"\n",
    "\n",
    "        parameter_iterable = ParameterGrid(self.param_grid)\n",
    "\n",
    "        estimator = self.estimator\n",
    "        cv = self.cv\n",
    "\n",
    "        n_samples = _num_samples(X)\n",
    "        X, y = indexable(X, y)\n",
    "\n",
    "        if y is not None:\n",
    "            if len(y) != n_samples:\n",
    "                raise ValueError('Target variable (y) has a different number '\n",
    "                                 'of samples (%i) than data (X: %i samples)'\n",
    "                                 % (len(y), n_samples))\n",
    "#         cv = check_cv(cv, X, y, classifier=is_classifier(estimator))\n",
    "        if self.verbose > 0:\n",
    "#             if isinstance(parameter_iterable, Sized):\n",
    "            n_candidates = len(parameter_iterable)\n",
    "            print(\"Fitting {0} folds for each of {1} candidates, totalling\"\n",
    "                  \" {2} fits\".format(len(cv), n_candidates,\n",
    "                                     n_candidates * len(cv)))\n",
    "\n",
    "        base_estimator = clone(self.estimator)\n",
    "\n",
    "        pre_dispatch = self.pre_dispatch\n",
    "\n",
    "        out = Parallel(\n",
    "                n_jobs=self.n_jobs, verbose=self.verbose,\n",
    "                pre_dispatch=pre_dispatch\n",
    "        )(\n",
    "                delayed(cv_fit_and_score)(clone(base_estimator), X, y, self.scoring,\n",
    "                                          parameters, cv=cv)\n",
    "                for parameters in parameter_iterable)\n",
    "#         print(out)\n",
    "        best = sorted(out,key=lambda x: x[0], reverse=True)[0]\n",
    "        self.best_params_ = best[1]\n",
    "        self.best_score_ = best[0]\n",
    "\n",
    "        if self.refit:\n",
    "            # fit the best estimator using the entire dataset\n",
    "            # clone first to work around broken estimators\n",
    "            best_estimator = clone(base_estimator).set_params(\n",
    "                    **best[1])\n",
    "#             if y is not None:\n",
    "#                 best_estimator.fit(X, y, **self.fit_params)\n",
    "#             else:\n",
    "#                 best_estimator.fit(X, **self.fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 60 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done  42 out of  60 | elapsed:  1.8min remaining:   46.2s\n",
      "[Parallel(n_jobs=40)]: Done  60 out of  60 | elapsed:  2.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModifiedGridSearchCV(cv=[(array([   0,    1,    2, ..., 2786, 2787, 2788]),\n",
       "                          array([427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439,\n",
       "       440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452,\n",
       "       453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465,\n",
       "       466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478,\n",
       "       479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491,\n",
       "       492, 493, 494, 495, 496, 497...\n",
       "                     param_grid={'rf__C': array([  1., 100.]),\n",
       "                                 'rf__cache_size': [2000],\n",
       "                                 'rf__class_weight': [{0: 0.0, 1: 1.0},\n",
       "                                                      {0: 0.2, 1: 0.8},\n",
       "                                                      {0: 0.4, 1: 0.6}],\n",
       "                                 'rf__gamma': array([1.e-09, 1.e-07, 1.e-05, 1.e-03, 1.e-01, 1.e+01, 1.e+03, 1.e+05,\n",
       "       1.e+07, 1.e+09]),\n",
       "                                 'rf__kernel': ['rbf'],\n",
       "                                 'rf__probability': [True],\n",
       "                                 'rf__verbose': [False]},\n",
       "                     pre_dispatch='2*n_jobs', refit=True,\n",
       "                     scoring=<function f1Bias_scorer_CV at 0x7fd997aa1730>,\n",
       "                     verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gkf = GroupKFold(n_splits=len(np.unique(groups)))\n",
    "# X1 = StandardScaler().fit_transform(X)\n",
    "X1 = X\n",
    "delta = 0.2\n",
    "parameters1 = {'rf__kernel': ['rbf'],\n",
    "              'rf__C': np.logspace(0,2,2),\n",
    "              'rf__gamma': np.logspace(-9,9,10),\n",
    "              'rf__class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.0, .50, delta)],\n",
    "              'rf__probability':[True],\n",
    "              'rf__verbose':[False],\n",
    "              'rf__cache_size':[2000]}\n",
    "parameters = {\n",
    "    'rf__min_samples_leaf': [4],\n",
    "    'rf__max_features': [.7,1],\n",
    "    'rf__n_estimators': [100],\n",
    "    'rf__n_jobs': [-1],\n",
    "    'rf__criterion':['gini','entropy'],\n",
    "    'rf__class_weight': [{0: w, 1: 1 - w} for w in np.arange(0.01, .5, delta)],\n",
    "    'rf__random_state': [42]\n",
    "       }\n",
    "svc = Pipeline([('sts',preprocessing.StandardScaler()),('rf',SVC())])\n",
    "# svc = RandomForestClassifier()\n",
    "# grid_search = GridSearchCV(svc,parameters, cv=gkf.split(X1,y,groups=groups), \n",
    "#              n_jobs=-1, scoring='f1', verbose=1, iid=False)\n",
    "# clf = Pipeline([('sts',StandardScaler()),('clf',svc)])\n",
    "grid_search = ModifiedGridSearchCV(svc, parameters1, cv=list(gkf.split(X1,y,groups=groups)),\n",
    "                                   n_jobs=40, scoring=f1Bias_scorer_CV, verbose=1, iid=False)\n",
    "grid_search.fit(X1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQvklEQVR4nO3dfYzlVX3H8fdHVrA+AbJTQne3XRrXtpS2kUwQY2KtaxXQsCRVAqllpZtuatFaMdW1/kGjMYHYSiWx2q1Ql8YilNqyqVhKAEPadCmDWORBZYo87BbcUR76QHxAv/3jHnRcZ5iHO3Nnh/N+JZM5v3POvb9zdmY/93fP73d/k6pCktSHZ630ACRJo2PoS1JHDH1J6oihL0kdMfQlqSNrVnoAT2ft2rW1cePGlR6GJK0qt9566zeqamymtoM69Ddu3MjExMRKD0OSVpUk98/W5vKOJHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR15KD+RO6wNu747Iz1913w+hGPRJIODh7pS1JHDH1J6oihL0kdMfQlqSNzhn6SS5PsT3LHtLoPJflyktuT/H2SI6a1vTfJZJKvJHndtPqTW91kkh1LPhNJ0pzmc6T/SeDkA+quA46vql8Gvgq8FyDJccCZwC+2x/x5kkOSHAJ8FDgFOA44q/WVJI3QnKFfVTcBjxxQ989V9WTb3AOsb+UtwKer6ttV9TVgEjixfU1W1b1V9R3g062vJGmElmJN/7eBz7XyOuDBaW17W91s9T8myfYkE0kmpqamlmB4kqSnDBX6Sd4HPAl8ammGA1W1s6rGq2p8bGzGP/EoSVqkRX8iN8lbgDcAm6uqWvU+YMO0butbHU9TL0kakUUd6Sc5GXg3cFpVPTGtaTdwZpLDkhwLbAL+HbgF2JTk2CSHMjjZu3u4oUuSFmrOI/0klwOvAtYm2Qucz+BqncOA65IA7Kmq362qO5NcCdzFYNnn3Kr6XnuetwHXAocAl1bVncswH0nS05gz9KvqrBmqL3ma/h8EPjhD/TXANQsanSRpSfmJXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkfmDP0klybZn+SOaXUvSnJdknva9yNbfZJcnGQyye1JTpj2mK2t/z1Jti7PdCRJT2c+R/qfBE4+oG4HcH1VbQKub9sApwCb2td24GMweJEAzgdeBpwInP/UC4UkaXTmDP2qugl45IDqLcCuVt4FnD6t/rIa2AMckeQY4HXAdVX1SFU9ClzHj7+QSJKW2WLX9I+uqoda+WHg6FZeBzw4rd/eVjdb/Y9Jsj3JRJKJqampRQ5PkjSToU/kVlUBtQRjeer5dlbVeFWNj42NLdXTSpJYfOh/vS3b0L7vb/X7gA3T+q1vdbPVS5JGaLGhvxt46gqcrcDV0+rPblfxnAQ83paBrgVem+TIdgL3ta1OkjRCa+bqkORy4FXA2iR7GVyFcwFwZZJtwP3AGa37NcCpwCTwBHAOQFU9kuQDwC2t3/ur6sCTw5KkZTZn6FfVWbM0bZ6hbwHnzvI8lwKXLmh0kqQl5SdyJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHRkq9JO8M8mdSe5IcnmS5yQ5NsnNSSaTXJHk0Nb3sLY92do3LskMJEnztujQT7IO+H1gvKqOBw4BzgQuBC6qqhcDjwLb2kO2AY+2+otaP0nSCA27vLMG+Ikka4DnAg8Brwauau27gNNbeUvbprVvTpIh9y9JWoBFh35V7QP+BHiAQdg/DtwKPFZVT7Zue4F1rbwOeLA99snW/6gDnzfJ9iQTSSampqYWOzxJ0gyGWd45ksHR+7HATwHPA04edkBVtbOqxqtqfGxsbNinkyRNM8zyzmuAr1XVVFV9F/gM8ArgiLbcA7Ae2NfK+4ANAK39cOCbQ+xfkrRAw4T+A8BJSZ7b1uY3A3cBNwJvbH22Ale38u62TWu/oapqiP1LkhZomDX9mxmckP0C8KX2XDuB9wDnJZlksGZ/SXvIJcBRrf48YMcQ45YkLcKaubvMrqrOB84/oPpe4MQZ+n4LeNMw+5MkDcdP5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjoyVOgnOSLJVUm+nOTuJC9P8qIk1yW5p30/svVNkouTTCa5PckJSzMFSdJ8DXuk/xHgn6rq54FfAe4GdgDXV9Um4Pq2DXAKsKl9bQc+NuS+JUkLtOjQT3I48ErgEoCq+k5VPQZsAXa1bruA01t5C3BZDewBjkhyzGL3L0lauGGO9I8FpoC/SnJbkk8keR5wdFU91Po8DBzdyuuAB6c9fm+r+xFJtieZSDIxNTU1xPAkSQcaJvTXACcAH6uqlwL/xw+XcgCoqgJqIU9aVTuraryqxsfGxoYYniTpQMOE/l5gb1Xd3LavYvAi8PWnlm3a9/2tfR+wYdrj17c6SdKILDr0q+ph4MEkP9eqNgN3AbuBra1uK3B1K+8Gzm5X8ZwEPD5tGUiSNAJrhnz824FPJTkUuBc4h8ELyZVJtgH3A2e0vtcApwKTwBOtryRphIYK/ar6IjA+Q9PmGfoWcO4w+5MkDcdP5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjoydOgnOSTJbUn+sW0fm+TmJJNJrkhyaKs/rG1PtvaNw+5bkrQwS3Gk/w7g7mnbFwIXVdWLgUeBba1+G/Boq7+o9ZMkjdBQoZ9kPfB64BNtO8Crgatal13A6a28pW3T2je3/pKkERn2SP/PgHcD32/bRwGPVdWTbXsvsK6V1wEPArT2x1v/H5Fke5KJJBNTU1NDDk+SNN2iQz/JG4D9VXXrEo6HqtpZVeNVNT42NraUTy1J3VszxGNfAZyW5FTgOcALgY8ARyRZ047m1wP7Wv99wAZgb5I1wOHAN4fYvyRpgRZ9pF9V762q9VW1ETgTuKGqfhO4EXhj67YVuLqVd7dtWvsNVVWL3b8kaeGW4zr99wDnJZlksGZ/Sau/BDiq1Z8H7FiGfUuSnsYwyzs/UFWfBz7fyvcCJ87Q51vAm5Zif5KkxfETuZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVkSe6nv9ps3PHZGevvu+D1Ix6JJI2WR/qS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjiw69JNsSHJjkruS3JnkHa3+RUmuS3JP+35kq0+Si5NMJrk9yQlLNQlJ0vwMc6T/JPCuqjoOOAk4N8lxwA7g+qraBFzftgFOATa1r+3Ax4bYtyRpERYd+lX1UFV9oZX/B7gbWAdsAXa1bruA01t5C3BZDewBjkhyzGL3L0lauCVZ00+yEXgpcDNwdFU91JoeBo5u5XXAg9MetrfVSZJGZOjQT/J84O+AP6iq/57eVlUF1AKfb3uSiSQTU1NTww5PkjTNUKGf5NkMAv9TVfWZVv31p5Zt2vf9rX4fsGHaw9e3uh9RVTuraryqxsfGxoYZniTpAMNcvRPgEuDuqvrwtKbdwNZW3gpcPa3+7HYVz0nA49OWgSRJIzDM/fRfAfwW8KUkX2x1fwRcAFyZZBtwP3BGa7sGOBWYBJ4Azhli35KkRVh06FfVvwCZpXnzDP0LOHex+5MkDa/Lv5w1G/+ilqRnOm/DIEkdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakj3oZhHrw9g6RnCo/0Jakjhr4kdcTQl6SOGPqS1BFP5A7BE7ySVhuP9CWpI4a+JHXE5Z1l4LKPpIOVR/qS1BGP9EfIdwCSDjTqXDD0DwJL+UP3hUUajYX+X5ut/6gZ+gcxA1xaWk8XvEsV1gdLuM/G0F+FFvNLtdDHLPQ/gC9EP3Sw/RstJuh6dLCH9VIZeegnORn4CHAI8ImqumDUY9DcVuro5mB8a7xSY1rJF49eArBHqarR7Sw5BPgq8OvAXuAW4Kyqumum/uPj4zUxMbHo/fmLK2m1GubFPcmtVTU+U9uoL9k8EZisqnur6jvAp4EtIx6DJHVr1Ms764AHp23vBV42vUOS7cD2tvm/Sb4yxP7WAt8Y4vGrUW9z7m2+4Jy7kAuHmvPPzNZw0J3IraqdwM6leK4kE7O9xXmm6m3Ovc0XnHMvlmvOo17e2QdsmLa9vtVJkkZg1KF/C7ApybFJDgXOBHaPeAyS1K2RLu9U1ZNJ3gZcy+CSzUur6s5l3OWSLBOtMr3Nubf5gnPuxbLMeaSXbEqSVpZ32ZSkjhj6ktSRVR/6SU5O8pUkk0l2zNB+WJIrWvvNSTauwDCX1DzmfF6Su5LcnuT6JLNes7tazDXnaf1+I0klWfWX981nzknOaD/rO5P8zajHuNTm8bv900luTHJb+/0+dSXGuVSSXJpkf5I7ZmlPkovbv8ftSU4YeqdVtWq/GJwM/k/gZ4FDgf8Ajjugz+8BH2/lM4ErVnrcI5jzrwHPbeW39jDn1u8FwE3AHmB8pcc9gp/zJuA24Mi2/ZMrPe4RzHkn8NZWPg64b6XHPeScXwmcANwxS/upwOeAACcBNw+7z9V+pD+f2zpsAXa18lXA5iQZ4RiX2pxzrqobq+qJtrmHwechVrP53r7jA8CFwLdGObhlMp85/w7w0ap6FKCq9o94jEttPnMu4IWtfDjwXyMc35KrqpuAR56myxbgshrYAxyR5Jhh9rnaQ3+m2zqsm61PVT0JPA4cNZLRLY/5zHm6bQyOFFazOefc3vZuqKpnyl325vNzfgnwkiT/mmRPu4PtajafOf8x8OYke4FrgLePZmgrZqH/3+d00N2GQUsnyZuBceBXV3osyynJs4APA29Z4aGM2hoGSzyvYvBu7qYkv1RVj63koJbZWcAnq+pPk7wc+Oskx1fV91d6YKvFaj/Sn89tHX7QJ8kaBm8JvzmS0S2Ped3KIslrgPcBp1XVt0c0tuUy15xfABwPfD7JfQzWPnev8pO58/k57wV2V9V3q+prDG5bvmlE41sO85nzNuBKgKr6N+A5DG7G9ky15LeuWe2hP5/bOuwGtrbyG4Ebqp0hWaXmnHOSlwJ/wSDwV/s6L8wx56p6vKrWVtXGqtrI4DzGaVW1+D/GsPLm87v9DwyO8kmylsFyz70jHONSm8+cHwA2AyT5BQahPzXSUY7WbuDsdhXPScDjVfXQME+4qpd3apbbOiR5PzBRVbuBSxi8BZxkcMLkzJUb8fDmOecPAc8H/rads36gqk5bsUEPaZ5zfkaZ55yvBV6b5C7ge8AfVtWqfRc7zzm/C/jLJO9kcFL3Lav5IC7J5QxeuNe28xTnA88GqKqPMzhvcSowCTwBnDP0Plfxv5ckaYFW+/KOJGkBDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkf8HhmCxWqfeckQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7776824034334765 0.3015185786630919\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      2254\n",
      "           1       0.72      0.85      0.78       535\n",
      "\n",
      "    accuracy                           0.91      2789\n",
      "   macro avg       0.84      0.88      0.86      2789\n",
      "weighted avg       0.92      0.91      0.91      2789\n",
      "\n",
      "[[2077  177]\n",
      " [  82  453]]\n",
      "0.8032258064516129\n"
     ]
    }
   ],
   "source": [
    "clf = grid_search.best_estimator_\n",
    "clf\n",
    "m = len(np.where(y==0)[0])\n",
    "n = len(np.where(y>0)[0])\n",
    "clf.probability = True\n",
    "CV_probs = cross_val_probs(clf, X1, y, gkf.split(X1,y,groups=groups))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(CV_probs,50)\n",
    "plt.show()\n",
    "# score, bias = Twobias_scorer_CV(CV_probs, y, True)\n",
    "score, bias = f1Bias_scorer_CV(CV_probs, y, True)\n",
    "predicted = np.asarray(CV_probs >= bias, dtype=np.int)\n",
    "classified = range(n)\n",
    "print(score,bias)\n",
    "\n",
    "f = np.zeros((len(y),2))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "print(metrics.classification_report(y, predicted))\n",
    "print(metrics.confusion_matrix(y, predicted))\n",
    "\n",
    "data['groups'] = groups\n",
    "data['original'] = [[i] for i in y]\n",
    "data['predicted'] = [[i] for i in predicted]\n",
    "f_scores = []\n",
    "data = data.groupby('groups').sum()\n",
    "for i in range(data.shape[0]):\n",
    "    f_scores.append(f1_score(data['original'][i],data['predicted'][i]))\n",
    "print(np.median(f_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X1,y)\n",
    "\n",
    "pickle.dump(clf,open('/home/jupyter/mullah/apply_cstress_to_rice/models/rip_model_final.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CC3.3",
   "language": "python",
   "name": "cc33"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
